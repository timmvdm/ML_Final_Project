{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d04b16ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "febbb54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pr13_stocks (1).csv', index_col=0)\n",
    "# df = df.sample(frac=0.05, random_state=42)  \n",
    "# df = df[df['Ticker']=='CL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66021bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 12)\n",
      "Date             object\n",
      "Dividends       float64\n",
      "Stock Splits    float64\n",
      "Brand_Name       object\n",
      "Ticker           object\n",
      "Industry_Tag     object\n",
      "Country          object\n",
      "Volume          float64\n",
      "Open            float64\n",
      "High            float64\n",
      "Low             float64\n",
      "Close           float64\n",
      "dtype: object\n",
      "                        Date  Dividends  Stock Splits  \\\n",
      "0  2021-01-25 00:00:00-05:00        0.0           0.0   \n",
      "1  2019-09-12 00:00:00-04:00        0.0           0.0   \n",
      "2  2015-12-29 00:00:00-05:00        0.0           0.0   \n",
      "3  2014-06-13 00:00:00-04:00        0.0           0.0   \n",
      "4  2017-10-06 00:00:00-04:00        0.0           0.0   \n",
      "\n",
      "                Brand_Name Ticker    Industry_Tag      Country      Volume  \\\n",
      "0                    crocs   CROX        footwear          usa   1102500.0   \n",
      "1                   target    TGT          retail          usa   3185700.0   \n",
      "2                 unilever     UL  consumer goods  netherlands   1278700.0   \n",
      "3                      amd    AMD      technology          usa  17734600.0   \n",
      "4  the walt disney company    DIS   entertainment          usa   4360200.0   \n",
      "\n",
      "         Open        High        Low       Close  \n",
      "0   73.180000   74.750000  71.050003   73.910004  \n",
      "1  100.810594  101.077753  99.999911  100.359192  \n",
      "2   33.785507   33.970634  33.700657   33.908924  \n",
      "3    4.360000    4.390000   4.240000    4.280000  \n",
      "4   96.426167   96.734885  95.837672   96.541939  \n",
      "Number of duplicates: 0\n",
      "Number of Missing Values: 4261\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 100000 entries, 0 to 99999\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Date          99984 non-null  object \n",
      " 1   Dividends     99941 non-null  float64\n",
      " 2   Stock Splits  99542 non-null  float64\n",
      " 3   Brand_Name    99775 non-null  object \n",
      " 4   Ticker        99825 non-null  object \n",
      " 5   Industry_Tag  99966 non-null  object \n",
      " 6   Country       99900 non-null  object \n",
      " 7   Volume        99148 non-null  float64\n",
      " 8   Open          99197 non-null  float64\n",
      " 9   High          99336 non-null  float64\n",
      " 10  Low           99319 non-null  float64\n",
      " 11  Close         99806 non-null  float64\n",
      "dtypes: float64(7), object(5)\n",
      "memory usage: 9.9+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>99941.000000</td>\n",
       "      <td>99542.000000</td>\n",
       "      <td>9.914800e+04</td>\n",
       "      <td>99197.000000</td>\n",
       "      <td>99336.000000</td>\n",
       "      <td>99319.000000</td>\n",
       "      <td>99806.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.003058</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>2.347898e+07</td>\n",
       "      <td>65.486750</td>\n",
       "      <td>66.153202</td>\n",
       "      <td>64.724860</td>\n",
       "      <td>65.467484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.044493</td>\n",
       "      <td>0.050264</td>\n",
       "      <td>8.820551e+07</td>\n",
       "      <td>118.103474</td>\n",
       "      <td>119.203651</td>\n",
       "      <td>116.686773</td>\n",
       "      <td>117.964329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.199012</td>\n",
       "      <td>0.199922</td>\n",
       "      <td>0.196739</td>\n",
       "      <td>0.199164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.354575e+06</td>\n",
       "      <td>15.060389</td>\n",
       "      <td>15.268422</td>\n",
       "      <td>14.853267</td>\n",
       "      <td>15.043717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.621250e+06</td>\n",
       "      <td>32.457324</td>\n",
       "      <td>32.835369</td>\n",
       "      <td>32.076428</td>\n",
       "      <td>32.505058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.228455e+07</td>\n",
       "      <td>72.571576</td>\n",
       "      <td>73.143646</td>\n",
       "      <td>71.884888</td>\n",
       "      <td>72.491188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.012000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.768427e+09</td>\n",
       "      <td>2152.699951</td>\n",
       "      <td>2173.629883</td>\n",
       "      <td>2119.989990</td>\n",
       "      <td>2153.199951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Dividends  Stock Splits        Volume          Open          High  \\\n",
       "count  99941.000000  99542.000000  9.914800e+04  99197.000000  99336.000000   \n",
       "mean       0.003058      0.000536  2.347898e+07     65.486750     66.153202   \n",
       "std        0.044493      0.050264  8.820551e+07    118.103474    119.203651   \n",
       "min        0.000000      0.000000  0.000000e+00      0.199012      0.199922   \n",
       "25%        0.000000      0.000000  1.354575e+06     15.060389     15.268422   \n",
       "50%        0.000000      0.000000  4.621250e+06     32.457324     32.835369   \n",
       "75%        0.000000      0.000000  1.228455e+07     72.571576     73.143646   \n",
       "max        2.012000     10.000000  2.768427e+09   2152.699951   2173.629883   \n",
       "\n",
       "                Low         Close  \n",
       "count  99319.000000  99806.000000  \n",
       "mean      64.724860     65.467484  \n",
       "std      116.686773    117.964329  \n",
       "min        0.196739      0.199164  \n",
       "25%       14.853267     15.043717  \n",
       "50%       32.076428     32.505058  \n",
       "75%       71.884888     72.491188  \n",
       "max     2119.989990   2153.199951  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x000001B23909B9C0> (for post_execute), with arguments args (),kwargs {}:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\matplotlib_inline\\backend_inline.py:126\u001b[0m, in \u001b[0;36mflush_figures\u001b[1;34m()\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m InlineBackend\u001b[38;5;241m.\u001b[39minstance()\u001b[38;5;241m.\u001b[39mclose_figures:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# ignore the tracking, just draw and close all figures\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m show(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;66;03m# safely show traceback if in IPython, else raise\u001b[39;00m\n\u001b[0;32m    129\u001b[0m         ip \u001b[38;5;241m=\u001b[39m get_ipython()\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\matplotlib_inline\\backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(close, block)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[1;32m---> 90\u001b[0m         display(\n\u001b[0;32m     91\u001b[0m             figure_manager\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mfigure,\n\u001b[0;32m     92\u001b[0m             metadata\u001b[38;5;241m=\u001b[39m_fetch_figure_metadata(figure_manager\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mfigure)\n\u001b[0;32m     93\u001b[0m         )\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\IPython\\core\\display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mformat\u001b[39m(obj, include\u001b[38;5;241m=\u001b[39minclude, exclude\u001b[38;5;241m=\u001b[39mexclude)\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\IPython\\core\\formatters.py:238\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    236\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 238\u001b[0m     data \u001b[38;5;241m=\u001b[39m formatter(obj)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\IPython\\core\\formatters.py:282\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[1;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     r \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\IPython\\core\\formatters.py:402\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m printer(obj)\n\u001b[0;32m    403\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    404\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170\u001b[0m, in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[0;32m    168\u001b[0m     FigureCanvasBase(fig)\n\u001b[1;32m--> 170\u001b[0m fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mprint_figure(bytes_io, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    171\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\matplotlib\\backend_bases.py:2155\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2152\u001b[0m     \u001b[38;5;66;03m# we do this instead of `self.figure.draw_without_rendering`\u001b[39;00m\n\u001b[0;32m   2153\u001b[0m     \u001b[38;5;66;03m# so that we can inject the orientation\u001b[39;00m\n\u001b[0;32m   2154\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(renderer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_draw_disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, nullcontext)():\n\u001b[1;32m-> 2155\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m   2156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[0;32m   2157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\matplotlib\\artist.py:94\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 94\u001b[0m     result \u001b[38;5;241m=\u001b[39m draw(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[0;32m     96\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\matplotlib\\artist.py:71\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\matplotlib\\figure.py:3257\u001b[0m, in \u001b[0;36mFigure.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3254\u001b[0m             \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m   3256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m-> 3257\u001b[0m     mimage\u001b[38;5;241m.\u001b[39m_draw_list_compositing_images(\n\u001b[0;32m   3258\u001b[0m         renderer, \u001b[38;5;28mself\u001b[39m, artists, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppressComposite)\n\u001b[0;32m   3260\u001b[0m     renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   3261\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\matplotlib\\image.py:134\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[1;32m--> 134\u001b[0m         a\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\matplotlib\\artist.py:71\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\matplotlib\\axes\\_base.py:3181\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[0;32m   3179\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_figure(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), artists_rasterized, renderer)\n\u001b[1;32m-> 3181\u001b[0m mimage\u001b[38;5;241m.\u001b[39m_draw_list_compositing_images(\n\u001b[0;32m   3182\u001b[0m     renderer, \u001b[38;5;28mself\u001b[39m, artists, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_figure(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msuppressComposite)\n\u001b[0;32m   3184\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   3185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\matplotlib\\image.py:134\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[1;32m--> 134\u001b[0m         a\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\matplotlib\\artist.py:71\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\matplotlib\\axis.py:1416\u001b[0m, in \u001b[0;36mAxis.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   1413\u001b[0m renderer\u001b[38;5;241m.\u001b[39mopen_group(\u001b[38;5;18m__name__\u001b[39m, gid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_gid())\n\u001b[0;32m   1415\u001b[0m ticks_to_draw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_ticks()\n\u001b[1;32m-> 1416\u001b[0m tlb1, tlb2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ticklabel_bboxes(ticks_to_draw, renderer)\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks_to_draw:\n\u001b[0;32m   1419\u001b[0m     tick\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\matplotlib\\axis.py:1343\u001b[0m, in \u001b[0;36mAxis._get_ticklabel_bboxes\u001b[1;34m(self, ticks, renderer)\u001b[0m\n\u001b[0;32m   1341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1342\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_figure(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[1;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ([tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[0;32m   1344\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_visible()],\n\u001b[0;32m   1345\u001b[0m         [tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[0;32m   1346\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_visible()])\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\matplotlib\\text.py:969\u001b[0m, in \u001b[0;36mText.get_window_extent\u001b[1;34m(self, renderer, dpi)\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get window extent of text w/o renderer. You likely \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    966\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwant to call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure.draw_without_rendering()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(fig, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m--> 969\u001b[0m     bbox, info, descent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_layout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_renderer)\n\u001b[0;32m    970\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_unitless_position()\n\u001b[0;32m    971\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_transform()\u001b[38;5;241m.\u001b[39mtransform((x, y))\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\matplotlib\\text.py:382\u001b[0m, in \u001b[0;36mText._get_layout\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m    380\u001b[0m clean_line, ismath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_math(line)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_line:\n\u001b[1;32m--> 382\u001b[0m     w, h, d \u001b[38;5;241m=\u001b[39m _get_text_metrics_with_cache(\n\u001b[0;32m    383\u001b[0m         renderer, clean_line, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fontproperties,\n\u001b[0;32m    384\u001b[0m         ismath\u001b[38;5;241m=\u001b[39mismath, dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_figure(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mdpi)\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     w \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m=\u001b[39m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\matplotlib\\text.py:69\u001b[0m, in \u001b[0;36m_get_text_metrics_with_cache\u001b[1;34m(renderer, text, fontprop, ismath, dpi)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call ``renderer.get_text_width_height_descent``, caching the results.\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Cached based on a copy of fontprop so that later in-place mutations of\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# the passed-in argument do not mess up the cache.\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_text_metrics_with_cache_impl(\n\u001b[0;32m     70\u001b[0m     weakref\u001b[38;5;241m.\u001b[39mref(renderer), text, fontprop\u001b[38;5;241m.\u001b[39mcopy(), ismath, dpi)\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\matplotlib\\text.py:77\u001b[0m, in \u001b[0;36m_get_text_metrics_with_cache_impl\u001b[1;34m(renderer_ref, text, fontprop, ismath, dpi)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(\u001b[38;5;241m4096\u001b[39m)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_text_metrics_with_cache_impl\u001b[39m(\n\u001b[0;32m     75\u001b[0m         renderer_ref, text, fontprop, ismath, dpi):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m# dpi is unused, but participates in cache invalidation (via the renderer).\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m renderer_ref()\u001b[38;5;241m.\u001b[39mget_text_width_height_descent(text, fontprop, ismath)\n",
      "File \u001b[1;32mc:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:219\u001b[0m, in \u001b[0;36mRendererAgg.get_text_width_height_descent\u001b[1;34m(self, s, prop, ismath)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m width, height, descent\n\u001b[0;32m    218\u001b[0m font \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_font(prop)\n\u001b[1;32m--> 219\u001b[0m font\u001b[38;5;241m.\u001b[39mset_text(s, \u001b[38;5;241m0.0\u001b[39m, flags\u001b[38;5;241m=\u001b[39mget_hinting_flag())\n\u001b[0;32m    220\u001b[0m w, h \u001b[38;5;241m=\u001b[39m font\u001b[38;5;241m.\u001b[39mget_width_height()  \u001b[38;5;66;03m# width and height of unrotated string\u001b[39;00m\n\u001b[0;32m    221\u001b[0m d \u001b[38;5;241m=\u001b[39m font\u001b[38;5;241m.\u001b[39mget_descent()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#General Information about the Dataset\n",
    "print(df.shape) \n",
    "print(df.dtypes) \n",
    "print(df.head())\n",
    "# print('Close Price distribution:',df['Close'].value_counts(normalize=True)).plot(kind='bar')\n",
    "# df['Close'].plot(kind='bar')\n",
    "print('Number of duplicates:', np.sum(df.duplicated()))\n",
    "print('Number of Missing Values:', df.isnull().sum().sum())\n",
    "#General Descriptives about the Dataset\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fcaa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
    "#drop the 16 rows without any date data\n",
    "df = df.dropna(subset=['Date'])\n",
    "#drop the row without both Brand_name and Ticker as we cannot say which company this entry refers to\n",
    "df = df[~(df['Brand_Name'].isna() & df['Ticker'].isna())].copy()\n",
    "\n",
    "df['Brand_Name'] = df['Brand_Name'].str.lower()\n",
    "df['Ticker'] = df['Ticker'].str.upper()\n",
    "\n",
    "brand_to_ticker = df.dropna(subset=['Brand_Name', 'Ticker'])\\\n",
    "                        .drop_duplicates(subset=['Brand_Name'])\\\n",
    "                        .set_index('Brand_Name')['Ticker'].to_dict()\n",
    "\n",
    "ticker_to_brand = df.dropna(subset=['Brand_Name', 'Ticker'])\\\n",
    "                        .drop_duplicates(subset=['Ticker'])\\\n",
    "                        .set_index('Ticker')['Brand_Name'].to_dict()\n",
    "\n",
    "    # Fill missing Ticker using Brand_Name\n",
    "df.loc[df['Ticker'].isna() & df['Brand_Name'].notna(), 'Ticker'] = (\n",
    "        df.loc[df['Ticker'].isna() & df['Brand_Name'].notna(), 'Brand_Name']\n",
    "        .map(brand_to_ticker)\n",
    "    )\n",
    "\n",
    "    # Fill missing Brand_Name using Ticker\n",
    "df.loc[df['Brand_Name'].isna() & df['Ticker'].notna(), 'Brand_Name'] = (\n",
    "    df.loc[df['Brand_Name'].isna() & df['Ticker'].notna(), 'Ticker']\n",
    "    .map(ticker_to_brand))\n",
    "\n",
    "\n",
    "\n",
    "#As dividend payments and stock splits are sparse, event-based features, missing values here are likely because \n",
    "#no such event has occured. Therefore, missing values are filled with 0. Also, the occurence of missing values \n",
    "#is relatively low here(59 and 458, respectively) compared to the overall dataset size(~100,000)\n",
    "df['Dividends'] = df['Dividends'].fillna(0.0)\n",
    "df['Stock Splits'] = df['Stock Splits'].fillna(0.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Imputing industry tag values with the most frequent value for each brand\n",
    "\n",
    "df['Industry_Tag'] = df['Industry_Tag'].str.strip().str.lower()\n",
    "industry_map = (\n",
    "        df.dropna(subset=['Industry_Tag'])\n",
    "          .groupby('Brand_Name')['Industry_Tag']\n",
    "          .agg(lambda x: x.mode()[0]) \n",
    "          .to_dict()\n",
    "    )\n",
    "\n",
    "df['Industry_Tag'] = df.apply(\n",
    "        lambda row: industry_map[row['Brand_Name']]\n",
    "        if pd.isna(row['Industry_Tag']) or row['Industry_Tag'] != industry_map.get(row['Brand_Name'])\n",
    "        else row['Industry_Tag'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "#same for country\n",
    "\n",
    "df['Country'] = df['Country'].str.strip()\n",
    "\n",
    "country_map = (\n",
    "        df.dropna(subset=['Country'])\n",
    "          .groupby('Brand_Name')['Country']\n",
    "          .agg(lambda x: x.mode()[0]) \n",
    "          .to_dict()\n",
    "    )\n",
    "\n",
    "df['Country'] = df.apply(\n",
    "        lambda row: country_map[row['Brand_Name']]\n",
    "        if pd.isna(row['Country']) or row['Country'] != country_map.get(row['Brand_Name'])\n",
    "        else row['Country'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "#drop Open High Low since they give information at the same day as they are not independent of the Close price --> our target\n",
    "# drop Brand Name as it contains the same info as Ticker\n",
    "df = df.drop(columns=['Open', 'High', 'Low','Brand_Name'])\n",
    "#drop all nans in target and the corresponding rows -->hard to predict without target\n",
    "df = df.dropna(subset=['Close'])\n",
    "print(df.shape,'shape before vol dropped')\n",
    "#drop all nans in volumne and the corresponding rows\n",
    "df = df.dropna(subset=['Volume'])\n",
    "print(df.shape,'shape after vol dropped')\n",
    "\n",
    "\n",
    "#we decided not to exclude outliers, however they are defined, since we expect the models to learn from these 'special' circumstances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e0b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated = df[df.duplicated(subset=['Ticker', 'Date'], keep=False)]\n",
    "duplicated\n",
    "#drop all duplicates if the ticker and Date are the same\n",
    "df = df.drop_duplicates(subset=['Ticker', 'Date'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafc5a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date            0\n",
      "Dividends       0\n",
      "Stock Splits    0\n",
      "Ticker          0\n",
      "Industry_Tag    0\n",
      "Country         0\n",
      "Volume          0\n",
      "Close           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check\n",
    "print(df.isna().sum())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f3038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include new time features, since we expected to find some pattern of the time, e.g. an increase over the years (that we see), or an increase on certain days of the week or the year (cannot observe that).\n",
    "df['Date'] = pd.to_datetime(df['Date']) \n",
    "\n",
    "# Time-based features\n",
    "df['Year'] = df['Date'].dt.year.astype('int32')\n",
    "df['Month'] = df['Date'].dt.month.astype('int32')\n",
    "df['Day'] = df['Date'].dt.dayofyear.astype('int32')\n",
    "df['DayOfWeek'] = df['Date'].dt.dayofweek.astype('int32')  # 0 = Monday, 6 = Sunday\n",
    "\n",
    "# Running day number to determine the date on, rather than a datetime type\n",
    "df['Day_Number'] = (df['Date'] - df['Date'].min()).dt.days\n",
    "df = df.sort_values(by='Date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we wanted to add the past close prices, volumnes and the time distance to the current date in days to the df\n",
    "def add_lags(df, lags):\n",
    "    for lag in lags:\n",
    "        df[f'Close_Lag_{lag}' ] = df.groupby('Ticker' )['Close'].shift(lag)\n",
    "        df[f'Volume_Lag_{lag}' ] = df.groupby('Ticker')['Volume'].shift(lag)\n",
    "        df[f'Days_Since_Lag_{lag}' ] = df.groupby('Ticker')['Date'].diff(lag).dt.days\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec3ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we wanted to implement a time weighted mean and std to the df. because our initial function took over 30min we improved speed with numba\n",
    "from numba import njit\n",
    "\n",
    "#calculate the weighted mean and std using numba (dramatic time reduction)\n",
    "@njit\n",
    "def weighted_mean_std_numba(close_lags, days_diff):\n",
    "\n",
    "    if np.sum(days_diff) == 0:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    weight = 1.0/days_diff\n",
    "    weight_sum = np.sum(weight)\n",
    "\n",
    "\n",
    "    if weight_sum == 0.0 or np.any(np.isnan(weight)) or np.any(np.isinf(weight)):\n",
    "        return np.nan, np.nan\n",
    "\n",
    "\n",
    "\n",
    "    weighted_mean = np.sum(weight* close_lags) /weight_sum\n",
    "    weighted_var = np.sum(weight*(close_lags- weighted_mean)**2) /weight_sum\n",
    "    weighted_std = np.sqrt(weighted_var)\n",
    "\n",
    "    return weighted_mean, weighted_std\n",
    "\n",
    "def add_weighted_mean_std_numba(df, lookback_periods):\n",
    "    df = df.sort_values(['Ticker', 'Date']).copy()\n",
    "\n",
    "    for lookback in lookback_periods:\n",
    "        weighted_means = np.full(len(df),np.nan)\n",
    "        weighted_stds = np.full(len(df), np.nan)\n",
    "\n",
    "        idx = df.index.to_numpy()\n",
    "        closes = df['Close'].to_numpy()\n",
    "        dates = pd.to_datetime(df['Date']).values.astype('datetime64[D]')\n",
    "        tickers = df['Ticker'].to_numpy()\n",
    "\n",
    "        unique_tickers = np.unique(tickers)\n",
    "\n",
    "        for ticker in unique_tickers:\n",
    "\n",
    "            ticker_mask = tickers == ticker\n",
    "            ticker_indices = np.where(ticker_mask)[0]\n",
    "\n",
    "            for i in range(lookback, len(ticker_indices)):\n",
    "\n",
    "                idx_range = ticker_indices[i - lookback:i]\n",
    "                current_idx = ticker_indices[i]\n",
    "\n",
    "\n",
    "                close_lags = closes[idx_range]\n",
    "\n",
    "                date_lags = dates[idx_range]\n",
    "                current_date = dates[current_idx]\n",
    "                days_diff = (current_date - date_lags).astype(np.int64)\n",
    "\n",
    "\n",
    "\n",
    "                if np.any(days_diff == 0) or np.any(np.isnan(close_lags)):\n",
    "                    continue \n",
    "\n",
    "                mean_val, std_val = weighted_mean_std_numba(close_lags , days_diff)\n",
    "                weighted_means[current_idx] =mean_val\n",
    "                weighted_stds[current_idx] = std_val\n",
    "\n",
    "        df[f'Weighted_Mean_{lookback}']= weighted_means\n",
    "        df[f'Weighted_Std_{lookback}']= weighted_stds\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259464e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we also wanted to see how simply shifting the lookback period would influence the feature, without taking the timing differnce into account.\n",
    "def add_simple_mean_std(df, lookback_periods):\n",
    "    df = df.sort_values(['Ticker', 'Date']).copy()\n",
    "\n",
    "    for lookback in lookback_periods:\n",
    "\n",
    "        means =[]\n",
    "        stds =[]\n",
    "        for ticker, group in df.groupby('Ticker'):\n",
    "\n",
    "            group = group.sort_values('Date')\n",
    "            closes = group['Close_Lag_1'].reset_index(drop=True)\n",
    "            mean_vals = closes.rolling(window=lookback,min_periods=lookback).mean()\n",
    "            std_vals = closes.rolling(window=lookback, min_periods=lookback).std()\n",
    "\n",
    "            means.append(pd.Series(mean_vals.values, index=group.index))\n",
    "            stds.append(pd.Series(std_vals.values, index=group.index))\n",
    "\n",
    "\n",
    "        df[f'Simple_Mean_{lookback}'] =pd.concat(means)\n",
    "        df[f'Simple_Std_{lookback}'] =pd.concat(stds)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5a1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we added the return of the previous day\n",
    "def add_returns(df):\n",
    "    df['return_since_last_entry'] = df['Close_Lag_1']/df['Close_Lag_2'] -1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfe9610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we added the Close price difference of the previous day\n",
    "def add_diff(df):\n",
    "    df['diff_since_last_entry'] = df['Close_Lag_1']-df['Close_Lag_2']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daf3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since so many rows for dividends and stock splits are zero we wanted to add a signal for it to the df\n",
    "def add_cat_signals(df, cols):\n",
    "    for col in cols:\n",
    "        new_col = f\"{col}_signal\"\n",
    "        df[new_col] = (df[col] != 0).astype('int32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa876871",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_lags(df, [1,2,3,4,5,10,20,30,40,50,60,70,80,90,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4805c0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_weighted_mean_std_numba(df,[2,3,4,5,10,20,30,40,50,60,70,80,90,100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ccfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_simple_mean_std(df,[2,3,4,5,10,20,30,40,50,60,70,80,90,100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6237d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_returns(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13586a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_diff(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9008205",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_cat_signals(df, ['Dividends','Stock Splits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b039e549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Dividends', 'Stock Splits', 'Ticker', 'Industry_Tag',\n",
      "       'Country', 'Volume', 'Close', 'Year', 'Month',\n",
      "       ...\n",
      "       'Simple_Mean_80', 'Simple_Std_80', 'Simple_Mean_90', 'Simple_Std_90',\n",
      "       'Simple_Mean_100', 'Simple_Std_100', 'return_since_last_entry',\n",
      "       'diff_since_last_entry', 'Dividends_signal', 'Stock Splits_signal'],\n",
      "      dtype='object', length=118)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Date                          0\n",
       "Dividends                     0\n",
       "Stock Splits                  0\n",
       "Ticker                        0\n",
       "Industry_Tag                  0\n",
       "Country                       0\n",
       "Volume                        0\n",
       "Close                         0\n",
       "Year                          0\n",
       "Month                         0\n",
       "Day                           0\n",
       "DayOfWeek                     0\n",
       "Day_Number                    0\n",
       "Close_Lag_1                  61\n",
       "Volume_Lag_1                 61\n",
       "Days_Since_Lag_1             61\n",
       "Close_Lag_2                 122\n",
       "Volume_Lag_2                122\n",
       "Days_Since_Lag_2            122\n",
       "Close_Lag_3                 183\n",
       "Volume_Lag_3                183\n",
       "Days_Since_Lag_3            183\n",
       "Close_Lag_4                 244\n",
       "Volume_Lag_4                244\n",
       "Days_Since_Lag_4            244\n",
       "Close_Lag_5                 305\n",
       "Volume_Lag_5                305\n",
       "Days_Since_Lag_5            305\n",
       "Close_Lag_10                610\n",
       "Volume_Lag_10               610\n",
       "Days_Since_Lag_10           610\n",
       "Close_Lag_20               1220\n",
       "Volume_Lag_20              1220\n",
       "Days_Since_Lag_20          1220\n",
       "Close_Lag_30               1830\n",
       "Volume_Lag_30              1830\n",
       "Days_Since_Lag_30          1830\n",
       "Close_Lag_40               2440\n",
       "Volume_Lag_40              2440\n",
       "Days_Since_Lag_40          2440\n",
       "Close_Lag_50               3050\n",
       "Volume_Lag_50              3050\n",
       "Days_Since_Lag_50          3050\n",
       "Close_Lag_60               3660\n",
       "Volume_Lag_60              3660\n",
       "Days_Since_Lag_60          3660\n",
       "Close_Lag_70               4270\n",
       "Volume_Lag_70              4270\n",
       "Days_Since_Lag_70          4270\n",
       "Close_Lag_80               4880\n",
       "Volume_Lag_80              4880\n",
       "Days_Since_Lag_80          4880\n",
       "Close_Lag_90               5490\n",
       "Volume_Lag_90              5490\n",
       "Days_Since_Lag_90          5490\n",
       "Close_Lag_100              6100\n",
       "Volume_Lag_100             6100\n",
       "Days_Since_Lag_100         6100\n",
       "Weighted_Mean_2             122\n",
       "Weighted_Std_2              122\n",
       "Weighted_Mean_3             183\n",
       "Weighted_Std_3              183\n",
       "Weighted_Mean_4             244\n",
       "Weighted_Std_4              244\n",
       "Weighted_Mean_5             305\n",
       "Weighted_Std_5              305\n",
       "Weighted_Mean_10            610\n",
       "Weighted_Std_10             610\n",
       "Weighted_Mean_20           1220\n",
       "Weighted_Std_20            1220\n",
       "Weighted_Mean_30           1830\n",
       "Weighted_Std_30            1830\n",
       "Weighted_Mean_40           2440\n",
       "Weighted_Std_40            2440\n",
       "Weighted_Mean_50           3050\n",
       "Weighted_Std_50            3050\n",
       "Weighted_Mean_60           3660\n",
       "Weighted_Std_60            3660\n",
       "Weighted_Mean_70           4270\n",
       "Weighted_Std_70            4270\n",
       "Weighted_Mean_80           4880\n",
       "Weighted_Std_80            4880\n",
       "Weighted_Mean_90           5490\n",
       "Weighted_Std_90            5490\n",
       "Weighted_Mean_100          6100\n",
       "Weighted_Std_100           6100\n",
       "Simple_Mean_2               122\n",
       "Simple_Std_2                122\n",
       "Simple_Mean_3               183\n",
       "Simple_Std_3                183\n",
       "Simple_Mean_4               244\n",
       "Simple_Std_4                244\n",
       "Simple_Mean_5               305\n",
       "Simple_Std_5                305\n",
       "Simple_Mean_10              610\n",
       "Simple_Std_10               610\n",
       "Simple_Mean_20             1220\n",
       "Simple_Std_20              1220\n",
       "Simple_Mean_30             1830\n",
       "Simple_Std_30              1830\n",
       "Simple_Mean_40             2440\n",
       "Simple_Std_40              2440\n",
       "Simple_Mean_50             3050\n",
       "Simple_Std_50              3050\n",
       "Simple_Mean_60             3660\n",
       "Simple_Std_60              3660\n",
       "Simple_Mean_70             4270\n",
       "Simple_Std_70              4270\n",
       "Simple_Mean_80             4880\n",
       "Simple_Std_80              4880\n",
       "Simple_Mean_90             5490\n",
       "Simple_Std_90              5490\n",
       "Simple_Mean_100            6100\n",
       "Simple_Std_100             6100\n",
       "return_since_last_entry     122\n",
       "diff_since_last_entry       122\n",
       "Dividends_signal              0\n",
       "Stock Splits_signal           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(df.columns)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4adabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since date is in the datetime type and the information of time is preserved in the Day_number column we can delete it\n",
    "df = df.drop('Date', axis=1)\n",
    "#we need to delete Volumne since it contains information to some extend about the same day. therefore we cannot use it. remeber we still have a lagged volumne in the df\n",
    "df = df.drop('Volume', axis=1)\n",
    "# print(df['Stock Splits_signal'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a48b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('pr13_all_cols.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f23fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_ticks = 5\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.stripplot(data=df, x=\"Close\", y=\"Year\", hue=\"Ticker\", legend=False)\n",
    "\n",
    "# xmin, xmax = plt.xlim()\n",
    "# tick_positions = np.linspace(xmin, xmax, num_ticks)\n",
    "# tick_labels = [f\"{pos:.2f}\" for pos in tick_positions]\n",
    "\n",
    "# plt.title('Close vs Year')\n",
    "# plt.xticks(tick_positions, tick_labels)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.stripplot(data=df, x=\"Close\", y=\"Month\", hue=\"Ticker\", legend=False)\n",
    "# plt.title('Close vs Month')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.stripplot(data=df, x=\"Close\", y=\"Day\", hue=\"Ticker\", legend=False)\n",
    "# plt.title('Close vs Day')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.stripplot(data=df, x=\"Close\", y=\"DayOfWeek\", hue=\"Ticker\", legend=False)\n",
    "# plt.title('Close vs DayOfWeek')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994575ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.stripplot(x='Country', y='Close', data=df, jitter=True, palette='deep')\n",
    "# plt.title('Country vs Close - Stripplot')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.stripplot(x='Industry_Tag', y='Close', data=df, jitter=True, palette='muted')\n",
    "# plt.title('Industry vs Close - Stripplot')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.stripplot(x='Ticker', y='Close', data=df, jitter=True, palette='bright')\n",
    "# plt.title('Ticker vs Close - Stripplot')\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289fbfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 'pairplot of close, div, stock split, theri signal'\n",
    "# sns.pairplot(df[['Close', 'Dividends', 'Stock Splits', 'Dividends_signal', 'Stock Splits_signal']], diag_kind=\"kde\", corner=True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1667c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(max(df['Close']),min(df['Close']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd090f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d0a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def corr_per_ticker(df,n_features = 10):\n",
    "#         list = []\n",
    "#         for ticker, group in df.groupby('Ticker'):\n",
    "#                 corr_matrix = df[ticker].corr()\n",
    "#                 target_corr = corr_matrix['Close'].drop('Close').sort_values(ascending=False)\n",
    "#                 print(f\"\\n Top {n_features} for {ticker} features by Pearson correlation with Close:\\n\")\n",
    "#                 print(target_corr)\n",
    "#                 list += (ticker,)\n",
    "\n",
    "n=30\n",
    "def analyze_correlation(df, n_features=n):\n",
    "    # Keep numeric columns + Ticker separately\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'Close' not in numeric_cols:\n",
    "        raise ValueError(\"'Close' must be in your DataFrame and numeric.\")\n",
    "\n",
    "    if 'Ticker' not in df.columns:\n",
    "        raise ValueError(\"'Ticker' must be in your DataFrame.\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Step 1: Calculate correlations\n",
    "    for ticker, group in df.groupby('Ticker'):\n",
    "        if len(group) < 2:\n",
    "            continue  # skip tickers with not enough data\n",
    "\n",
    "        group_numeric = group[numeric_cols]  # <-- only numeric columns here!\n",
    "\n",
    "        corr_matrix = group_numeric.corr()\n",
    "        target_corr = corr_matrix['Close'].drop('Close').sort_values(ascending=False)\n",
    "\n",
    "        top_features = target_corr.head(n_features)\n",
    "\n",
    "        for rank, (feature_name, correlation) in enumerate(top_features.items(), start=1):\n",
    "            results.append({\n",
    "                'Ticker': ticker,\n",
    "                'Feature': feature_name,\n",
    "                'Correlation': correlation,\n",
    "                'Rank': rank\n",
    "            })\n",
    "\n",
    "    # Step 2: Create a nice DataFrame from results\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "results_df = analyze_correlation(df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f65362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f6a677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_features, counts = np.unique(results_df['Feature'], return_counts=True)\n",
    "\n",
    "# Combine the unique features and their counts into a DataFrame\n",
    "feature_distribution = pd.DataFrame({'Feature': unique_features, 'Count': counts})\n",
    "\n",
    "threshold = n/2  # Change this to whatever threshold you want\n",
    "\n",
    "# Filter features that meet the threshold condition\n",
    "filtered_distribution = feature_distribution[feature_distribution['Count'] > threshold]\n",
    "\n",
    "# Plot the distribution using a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(filtered_distribution['Feature'], filtered_distribution['Count'], color='skyblue')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Feature Distribution (Filtered by Count Threshold)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad11465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def analyze_mutual_info(df, n_features=n):\n",
    "    # Keep numeric columns + Ticker separately\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'Close' not in numeric_cols:\n",
    "        raise ValueError(\"'Close' must be in your DataFrame and numeric.\")\n",
    "\n",
    "    if 'Ticker' not in df.columns:\n",
    "        raise ValueError(\"'Ticker' must be in your DataFrame.\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Step 1: Calculate mutual information\n",
    "    for ticker, group in df.groupby('Ticker'):\n",
    "        if len(group) < 2:\n",
    "            continue  # skip tickers with not enough data\n",
    "\n",
    "        # Drop rows with NaN values in any numeric columns\n",
    "        group = group.dropna(subset=numeric_cols)\n",
    "\n",
    "        if len(group) < 2:  # Check again after dropping NaNs\n",
    "            continue\n",
    "\n",
    "        group_numeric = group[numeric_cols]  # <-- only numeric columns here!\n",
    "        \n",
    "        # Drop 'Close' and other non-predictive columns\n",
    "        X = group_numeric.drop('Close', axis=1)\n",
    "        y = group_numeric['Close']\n",
    "\n",
    "        # Calculate mutual information between 'Close' and other features\n",
    "        mutual_info = mutual_info_regression(X, y)\n",
    "\n",
    "        # Create a pandas Series to associate mutual information with feature names\n",
    "        mutual_info_series = pd.Series(mutual_info, index=X.columns)\n",
    "\n",
    "        # Sort by mutual information (descending) and pick the top n_features\n",
    "        top_features = mutual_info_series.sort_values(ascending=False).head(n_features)\n",
    "\n",
    "        for rank, (feature_name, mi_value) in enumerate(top_features.items(), start=1):\n",
    "            results.append({\n",
    "                'Ticker': ticker,\n",
    "                'Feature': feature_name,\n",
    "                'Mutual Information': mi_value,\n",
    "                'Rank': rank\n",
    "            })\n",
    "\n",
    "    # Step 2: Create a nice DataFrame from results\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29424a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_results_df = analyze_mutual_info(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf67e298",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_features, counts = np.unique(mi_results_df['Feature'], return_counts=True)\n",
    "\n",
    "# Combine the unique features and their counts into a DataFrame\n",
    "feature_distribution = pd.DataFrame({'Feature': unique_features, 'Count': counts})\n",
    "\n",
    "threshold = n/2  # Change this to whatever threshold you want\n",
    "\n",
    "# Filter features that meet the threshold condition\n",
    "filtered_distribution = feature_distribution[feature_distribution['Count'] > threshold]\n",
    "\n",
    "# Plot the distribution using a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(filtered_distribution['Feature'], filtered_distribution['Count'], color='skyblue')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Feature Distribution (Filtered by Count Threshold)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fdad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import plotly.express as px\n",
    "\n",
    "# def plot_interactive_pairplot_whole_df(df):\n",
    "#     # Create the pairplot using Plotly Express for the entire DataFrame\n",
    "#     fig = px.scatter_matrix(\n",
    "#         df,\n",
    "#         dimensions=df.select_dtypes(include='number').columns,  # Automatically selects numeric columns\n",
    "#         color='Ticker',  # Color points by ticker (for better distinction between tickers)\n",
    "#         title=\"Interactive Pairplot for All Tickers\",\n",
    "#         labels={col: col for col in df.select_dtypes(include='number').columns},  # Label the columns nicely\n",
    "#     )\n",
    "    \n",
    "#     # Show the plot\n",
    "#     fig.show()\n",
    "\n",
    "# # Example usage:\n",
    "# # Assuming 'df' is your DataFrame with columns like 'Ticker', 'Open', 'Close', 'High', 'Low', etc.\n",
    "# # df = pd.read_csv('your_data.csv')  # Load your data\n",
    "# plot_interactive_pairplot_whole_df(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377ab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import VarianceThreshold, mutual_info_regression\n",
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# def pre_training_analysis_v2(df, target='Close', top_n=6, drop_low_variance=True, variance_threshold=0.01, save_plots=False):\n",
    "#     \"\"\"\n",
    "#     Extended EDA for regression:\n",
    "#     - Drops low-variance features\n",
    "#     - Correlation + Mutual Info\n",
    "#     - Histograms, KDEs\n",
    "#     - Pairplots\n",
    "#     - Scatterplots\n",
    "#     - VIF check for multicollinearity\n",
    "    \n",
    "#     Parameters:\n",
    "#     - df (pd.DataFrame): Data\n",
    "#     - target (str): Target column\n",
    "#     - top_n (int): Number of top features to show\n",
    "#     - drop_low_variance (bool): Drop near-constant columns\n",
    "#     - variance_threshold (float): Threshold for variance filter\n",
    "#     - save_plots (bool): Optionally save key plots\n",
    "#     \"\"\"\n",
    "\n",
    "#     # 0. Numerical subset\n",
    "#     numeric_df = df.select_dtypes(include=['float64', 'int32', 'int64']).copy()\n",
    "\n",
    "#     # 1. Handle NaNs\n",
    "#     numeric_df = numeric_df.dropna()\n",
    "#     print(f\"✅ Dropped rows with NaNs. Data shape: {numeric_df.shape}\")\n",
    "\n",
    "#     # 2. Drop low-variance\n",
    "#     if drop_low_variance:\n",
    "#         features = numeric_df.drop(columns=[target], errors='ignore')\n",
    "#         selector = VarianceThreshold(threshold=variance_threshold)\n",
    "#         selector.fit(features)\n",
    "#         retained_columns = features.columns[selector.get_support()]\n",
    "#         dropped_columns = features.columns[~selector.get_support()]\n",
    "        \n",
    "#         print(f\"\\n⚠️ Dropping {len(dropped_columns)} low-variance features:\\n{dropped_columns.tolist()}\\n\")\n",
    "#         numeric_df = numeric_df[retained_columns.tolist() + [target]]\n",
    "\n",
    "#         # Variance Barplot\n",
    "#         plt.figure(figsize=(10, 4))\n",
    "#         sns.barplot(x=retained_columns, y=selector.variances_[selector.get_support()])\n",
    "#         plt.xticks(rotation=90)\n",
    "#         plt.title(\"Retained Feature Variances\")\n",
    "#         if save_plots:\n",
    "#             plt.savefig(\"retained_variances.png\", dpi=300, bbox_inches=\"tight\")\n",
    "#         plt.show()\n",
    "\n",
    "#     # 3. Correlation\n",
    "#     corr_matrix = numeric_df.corr()\n",
    "#     target_corr = corr_matrix[target].drop(target).sort_values(ascending=False)\n",
    "    \n",
    "#     print(f\"\\n📈 Top {top_n} features by Pearson correlation with '{target}':\\n\")\n",
    "#     print(target_corr)\n",
    "\n",
    "#     # 4. Mutual Information\n",
    "#     # X = numeric_df.drop(columns=[target])\n",
    "#     # y = numeric_df[target]\n",
    "#     # mi_scores = mutual_info_regression(X, y, random_state=42)\n",
    "#     # mi_series = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "#     # print(f\"\\n🧠 Top {top_n} features by Mutual Information with '{target}':\\n\")\n",
    "#     # print(mi_series.head(top_n))\n",
    "\n",
    "#     # # 5. VIF (Variance Inflation Factor) for multicollinearity\n",
    "#     # vif_data = pd.DataFrame()\n",
    "#     # vif_data['Feature'] = X.columns\n",
    "#     # vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "#     # vif_data = vif_data.sort_values(by='VIF', ascending=False)\n",
    "    \n",
    "#     # print(f\"\\n🧠 Top {top_n} features by VIF (higher = more multicollinearity):\\n\")\n",
    "#     # print(vif_data.head(top_n))\n",
    "\n",
    "#     # --- PLOTS ---\n",
    "#     # 6. Correlation heatmap\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     sns.heatmap(corr_matrix[[target]].sort_values(by=target, ascending=False), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "#     plt.title(f\"Correlation with '{target}'\", fontsize=14)\n",
    "#     if save_plots:\n",
    "#         plt.savefig(\"correlation_heatmap.png\", dpi=300, bbox_inches=\"tight\")\n",
    "#     plt.show()\n",
    "\n",
    "#     # 7. KDE + Histograms of top mutual info features\n",
    "#     top_features = target_corr.head(top_n).index.tolist()\n",
    "#     for feature in top_features:\n",
    "#         fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "#         sns.histplot(numeric_df[feature], kde=False, ax=ax[0], color='skyblue')\n",
    "#         ax[0].set_title(f\"Histogram of {feature}\")\n",
    "#         sns.kdeplot(numeric_df[feature], ax=ax[1], fill=True, color='orange')\n",
    "#         ax[1].set_title(f\"KDE of {feature}\")\n",
    "#         ax.set()\n",
    "#         if save_plots:\n",
    "#             fig.savefig(f\"hist_kde_{feature}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "#         plt.show()\n",
    "\n",
    "#     # 8. Scatterplots against target\n",
    "#     for feature in top_features:\n",
    "#         plt.figure(figsize=(8, 4))\n",
    "#         sns.scatterplot(x=numeric_df[feature], y=numeric_df[target], alpha=0.5)\n",
    "#         plt.title(f\"Scatterplot: {feature} vs {target}\")\n",
    "#         if save_plots:\n",
    "#             plt.savefig(f\"scatter_{feature}_vs_{target}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "#         plt.show()\n",
    "\n",
    "#     # 9. Pairplot of top 5 MI features + target\n",
    "#     pairplot_features = top_features[:min(5, len(top_features))] + [target]\n",
    "#     sns.pairplot(numeric_df[pairplot_features], diag_kind=\"kde\", corner=True)\n",
    "#     plt.suptitle(f\"Pairplot of Top {top_n} MI Features + '{target}'\", y=1.02)\n",
    "#     if save_plots:\n",
    "#         plt.savefig(\"pairplot_top_features.png\", dpi=300, bbox_inches=\"tight\")\n",
    "#     plt.show()\n",
    "\n",
    "#     print(\"\\n🎯 Analysis complete!\")\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf1e68b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pre_training_analysis_v2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pre_training_analysis_v2(df, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, drop_low_variance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pre_training_analysis_v2' is not defined"
     ]
    }
   ],
   "source": [
    "# pre_training_analysis_v2(df, top_n=10, drop_low_variance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057034da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
