{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here everyone pastes their individual working parts after working on them in their personal branch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eaf75d",
   "metadata": {},
   "source": [
    "<!--TABLE OF CONTENTS-->\n",
    "Contents:\n",
    "- [Introduction](#Introduction)\n",
    "  - [What business problem are you solving?](#What-business-problem-are-you-solving?)\n",
    "  - [What is the machine learning problem that you are solving?](#What-is-the-machine-learning-problem-that-you-are-solving?)\n",
    "- [Data exploration and preparation](#Data-exploration-and-preparation)\n",
    "- [Feature Engineering and ex ante Feature Selection](#Feature-Engineering-and-ex-ante-Feature-Selection)\n",
    "- [Modeling](#Modeling)\n",
    "  - [Linear Models](#Linear-Models)\n",
    "    - [Ridge](#Ridge)\n",
    "    - [Lasso](#Lasso)\n",
    "  - [Tree Models](#Tree-Models)\n",
    "    - [XGBoost](#XGBoost)\n",
    "    - [CatBoost](#CatBoost)\n",
    "    - [Random Forest](#Random-Forest)\n",
    "  - [Classification Models](#Classification-Models)\n",
    "    - [KNN](#KNN)\n",
    "  - [Neural Network](#Neural-Network)\n",
    "- [Model Evaluation](#Model-Evaluation)\n",
    "- [Model Interpretation](#Model-Interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0befaa64",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### Group number:\n",
    "### Group members: Timm Vordermark\n",
    "### Student IDs: 65983\n",
    "### Project name:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be0dfdc",
   "metadata": {},
   "source": [
    "## What business problem are you solving?\n",
    "- Please state clearly what business problem are you solving. (one sentence)\n",
    "- Elaborate why is this a relevant problem, and what can you do with the model output to create business value, i.e., how is the model output actionable. (2-3 paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eed325",
   "metadata": {},
   "source": [
    "## What is the machine learning problem that you are solving?\n",
    "- Please state clearly what is the ML problem. \n",
    "- If applicable state your target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2fb3ee",
   "metadata": {},
   "source": [
    "## Data exploration and preparation \n",
    "\n",
    "- How many data instances do you have?\n",
    "- Do you have duplicates?\n",
    "- How many features? What type are they?\n",
    "- If they are categorical, what categories they have, what is their frequency?\n",
    "- If they are numerical, what is their distribution?\n",
    "- Do you have outliers, and do you need to do anything about them?\n",
    "- What is the distribution of the target variable?\n",
    "- If you have a target, you can also check the relationship between the target and the variables.\n",
    "- Do you have missing data? If yes, how are you going to handle it?\n",
    "- Can you use the features in their original form, or do you need to alter them in some way?\n",
    "- What have you learned about your data? Is there anything that can help you in feature engineering or modeling?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d22320",
   "metadata": {},
   "source": [
    "In the following section we perform comprehensive cleaning an preprocessing on the given financial dataset. First,  the Date column is standadized to datetime format and rows with missing date values are removed. We also drop rows that have missing values in both Brand_Name and Ticker, as these are essential identifiers. The code then standardizes text by converting Brand_Name to lowercase and Ticker to uppercase. We built dictionaries to impute missing Ticker or Brand_Name values based on the other, using the existing mappings from Ticker/Brand_Name. Sparse event columns like Dividends and Stock Splits are filled with 0, assuming no events. For Industry_Tag and Country, the most common value per brand is used to fill missing or inconsistent entries. Finally, columns like Open, High, and Low—which correlate closely with the target Close price—are dropped to avoid data leakage, along with Brand_Name (redundant with Ticker). The dataset is further cleaned by dropping rows with missing target (Close) and feature (Volume) values to ensure high-quality inputs for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e72df1",
   "metadata": {},
   "source": [
    "# Begründung Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05d5e7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timmv\\anaconda3\\envs\\ml2025\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder,MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, TimeSeriesSplit\n",
    "from xgboost import XGBRegressor, plot_tree\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from catboost import CatBoostRegressor\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f492265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pr13_stocks (1).csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ce7804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#General Information about the Dataset\n",
    "print(df.shape) \n",
    "print(df.dtypes) \n",
    "print(df.head())\n",
    "sns.histplot(data=df, x='Close', bins='auto', kde=True)\n",
    "plt.show()\n",
    "print('Number of duplicates:', np.sum(df.duplicated()))\n",
    "print('Number of Missing Values:', df.isnull().sum().sum())\n",
    "#General Descriptives about the Dataset\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ad255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
    "#drop the 16 rows without any date data\n",
    "df = df.dropna(subset=['Date'])\n",
    "#drop the row without both Brand_name and Ticker as we cannot say which company this entry refers to\n",
    "df = df[~(df['Brand_Name'].isna() & df['Ticker'].isna())].copy()\n",
    "\n",
    "df['Brand_Name'] = df['Brand_Name'].str.lower()\n",
    "df['Ticker'] = df['Ticker'].str.upper()\n",
    "\n",
    "brand_to_ticker = df.dropna(subset=['Brand_Name', 'Ticker'])\\\n",
    "                        .drop_duplicates(subset=['Brand_Name'])\\\n",
    "                        .set_index('Brand_Name')['Ticker'].to_dict()\n",
    "\n",
    "ticker_to_brand = df.dropna(subset=['Brand_Name', 'Ticker'])\\\n",
    "                        .drop_duplicates(subset=['Ticker'])\\\n",
    "                        .set_index('Ticker')['Brand_Name'].to_dict()\n",
    "\n",
    "    # Fill missing Ticker using Brand_Name\n",
    "df.loc[df['Ticker'].isna() & df['Brand_Name'].notna(), 'Ticker'] = (\n",
    "        df.loc[df['Ticker'].isna() & df['Brand_Name'].notna(), 'Brand_Name']\n",
    "        .map(brand_to_ticker)\n",
    "    )\n",
    "\n",
    "    # Fill missing Brand_Name using Ticker\n",
    "df.loc[df['Brand_Name'].isna() & df['Ticker'].notna(), 'Brand_Name'] = (\n",
    "    df.loc[df['Brand_Name'].isna() & df['Ticker'].notna(), 'Ticker']\n",
    "    .map(ticker_to_brand))\n",
    "\n",
    "\n",
    "\n",
    "#As dividend payments and stock splits are sparse, event-based features, missing values here are likely because \n",
    "#no such event has occured. Therefore, missing values are filled with 0. Also, the occurence of missing values \n",
    "#is relatively low here(59 and 458, respectively) compared to the overall dataset size(~100,000)\n",
    "df['Dividends'] = df['Dividends'].fillna(0.0)\n",
    "df['Stock Splits'] = df['Stock Splits'].fillna(0.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Imputing industry tag values with the most frequent value for each brand\n",
    "\n",
    "df['Industry_Tag'] = df['Industry_Tag'].str.strip().str.lower()\n",
    "industry_map = (\n",
    "        df.dropna(subset=['Industry_Tag'])\n",
    "          .groupby('Brand_Name')['Industry_Tag']\n",
    "          .agg(lambda x: x.mode()[0]) \n",
    "          .to_dict()\n",
    "    )\n",
    "\n",
    "df['Industry_Tag'] = df.apply(\n",
    "        lambda row: industry_map[row['Brand_Name']]\n",
    "        if pd.isna(row['Industry_Tag']) or row['Industry_Tag'] != industry_map.get(row['Brand_Name'])\n",
    "        else row['Industry_Tag'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "#same for country\n",
    "\n",
    "df['Country'] = df['Country'].str.strip()\n",
    "\n",
    "country_map = (\n",
    "        df.dropna(subset=['Country'])\n",
    "          .groupby('Brand_Name')['Country']\n",
    "          .agg(lambda x: x.mode()[0]) \n",
    "          .to_dict()\n",
    "    )\n",
    "\n",
    "df['Country'] = df.apply(\n",
    "        lambda row: country_map[row['Brand_Name']]\n",
    "        if pd.isna(row['Country']) or row['Country'] != country_map.get(row['Brand_Name'])\n",
    "        else row['Country'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "#drop Open High Low since they give information at the same day as they are not independent of the Close price --> our target\n",
    "# drop Brand Name as it contains the same info as Ticker\n",
    "df = df.drop(columns=['Open', 'High', 'Low','Brand_Name'])\n",
    "#drop all nans in target and the corresponding rows -->hard to predict without target\n",
    "print(df.shape,'shape before Close dropped')\n",
    "df = df.dropna(subset=['Close']) #194 rows dropped\n",
    "print(df.shape,'shape before vol dropped')\n",
    "#drop all nans in volumne and the corresponding rows\n",
    "df = df.dropna(subset=['Volume']) #852 rows dropped\n",
    "print(df.shape,'shape after vol dropped')\n",
    "\n",
    "\n",
    "#we decided not to exclude outliers, however they are defined, since we expect the models to learn from these 'special' circumstances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b594d2f",
   "metadata": {},
   "source": [
    "# Outliers Dropped?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7450671",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated = df[df.duplicated(subset=['Ticker', 'Date'], keep=False)]\n",
    "duplicated\n",
    "#drop all duplicates if the ticker and Date are the same\n",
    "df = df.drop_duplicates(subset=['Ticker', 'Date'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6913d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "print(df.isna().sum())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9bff61",
   "metadata": {},
   "source": [
    "## Feature Engineering and ex ante Feature Selection\n",
    "Creating good features is probably the most important step in the machine learning process. \n",
    "This might involve doing:\n",
    "- transformations\n",
    "- aggregating over data points or over time and space, or finding differences (for example: differences between two monthly bills, time difference between two contacts with the client) \n",
    "- creating dummy (binary) variables\n",
    "- discretization\n",
    "\n",
    "Business insight is very relevant in this process. If it is possible you can also find additional relevant data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c43e56",
   "metadata": {},
   "source": [
    "As decribed above the idea behind feature engineering is to create new features that carry more predictive power and help the model to increase its performance.We start by extracting time-related features from the Date column, such as the year, month, day of year, and day of the week, under the assumption that stock performance may follow time patterns. Day_Number is also computed to represent time progression numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d713fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include new time features, since we expected to find some pattern of the time, e.g. an increase over the years (that we see), or an increase on certain days of the week or the year (cannot observe that).\n",
    "df['Date'] = pd.to_datetime(df['Date']) \n",
    "\n",
    "# Time-based features\n",
    "df['Year'] = df['Date'].dt.year.astype('int32')\n",
    "df['Month'] = df['Date'].dt.month.astype('int32')\n",
    "df['Day'] = df['Date'].dt.dayofyear.astype('int32')\n",
    "df['DayOfWeek'] = df['Date'].dt.dayofweek.astype('int32')  # 0 = Monday, 6 = Sunday\n",
    "\n",
    "# Running day number to determine the date on, rather than a datetime type\n",
    "df['Day_Number'] = (df['Date'] - df['Date'].min()).dt.days\n",
    "df = df.sort_values(by='Date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d1d296",
   "metadata": {},
   "source": [
    "The dataframe is enriched with lagged features using the add_lags function, which creating shifted versions of the Close price and Volume, as well as the number of days between the current and lagged observations. The idea is that the model can identify short-term historical trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1102d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we wanted to add the past close prices, volumnes and the time distance to the current date in days to the df\n",
    "def add_lags(df, lags):\n",
    "    for lag in lags:\n",
    "        df[f'Close_Lag_{lag}' ] = df.groupby('Ticker' )['Close'].shift(lag)\n",
    "        df[f'Volume_Lag_{lag}' ] = df.groupby('Ticker')['Volume'].shift(lag)\n",
    "        df[f'Days_Since_Lag_{lag}' ] = df.groupby('Ticker')['Date'].diff(lag).dt.days\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240f0af",
   "metadata": {},
   "source": [
    "To include time and price sensitive features we wanted to include a time weighted average and std. This ensures that more recent prices carry more weight, reflecting their likely higher relevance to current market behavior. At the same time simple unweighted rolling mean and std are computed for comparison using the add_simple_mean_std function, which helps to evaluate the effect of time-weighting on predictive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d196ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we wanted to implement a time weighted mean and std to the df. because our initial function took over 30min we improved speed with numba\n",
    "from numba import njit\n",
    "\n",
    "#calculate the weighted mean and std using numba (dramatic time reduction)\n",
    "@njit\n",
    "def weighted_mean_std_numba(close_lags, days_diff):\n",
    "\n",
    "    if np.sum(days_diff) == 0:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    weight = 1.0/days_diff\n",
    "    weight_sum = np.sum(weight)\n",
    "\n",
    "\n",
    "    if weight_sum == 0.0 or np.any(np.isnan(weight)) or np.any(np.isinf(weight)):\n",
    "        return np.nan, np.nan\n",
    "\n",
    "\n",
    "\n",
    "    weighted_mean = np.sum(weight* close_lags) /weight_sum\n",
    "    weighted_var = np.sum(weight*(close_lags- weighted_mean)**2) /weight_sum\n",
    "    weighted_std = np.sqrt(weighted_var)\n",
    "\n",
    "    return weighted_mean, weighted_std\n",
    "\n",
    "def add_weighted_mean_std_numba(df, lookback_periods):\n",
    "    df = df.sort_values(['Ticker', 'Date']).copy()\n",
    "\n",
    "    for lookback in lookback_periods:\n",
    "        weighted_means = np.full(len(df),np.nan)\n",
    "        weighted_stds = np.full(len(df), np.nan)\n",
    "\n",
    "        idx = df.index.to_numpy()\n",
    "        closes = df['Close'].to_numpy()\n",
    "        dates = pd.to_datetime(df['Date']).values.astype('datetime64[D]')\n",
    "        tickers = df['Ticker'].to_numpy()\n",
    "\n",
    "        unique_tickers = np.unique(tickers)\n",
    "\n",
    "        for ticker in unique_tickers:\n",
    "\n",
    "            ticker_mask = tickers == ticker\n",
    "            ticker_indices = np.where(ticker_mask)[0]\n",
    "\n",
    "            for i in range(lookback, len(ticker_indices)):\n",
    "\n",
    "                idx_range = ticker_indices[i - lookback:i]\n",
    "                current_idx = ticker_indices[i]\n",
    "\n",
    "\n",
    "                close_lags = closes[idx_range]\n",
    "\n",
    "                date_lags = dates[idx_range]\n",
    "                current_date = dates[current_idx]\n",
    "                days_diff = (current_date - date_lags).astype(np.int64)\n",
    "\n",
    "\n",
    "\n",
    "                if np.any(days_diff == 0) or np.any(np.isnan(close_lags)):\n",
    "                    continue \n",
    "\n",
    "                mean_val, std_val = weighted_mean_std_numba(close_lags , days_diff)\n",
    "                weighted_means[current_idx] =mean_val\n",
    "                weighted_stds[current_idx] = std_val\n",
    "\n",
    "        df[f'Weighted_Mean_{lookback}']= weighted_means\n",
    "        df[f'Weighted_Std_{lookback}']= weighted_stds\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we also wanted to see how simply shifting the lookback period would influence the feature, without taking the timing differnce into account.\n",
    "def add_simple_mean_std(df, lookback_periods):\n",
    "    df = df.sort_values(['Ticker', 'Date']).copy()\n",
    "\n",
    "    for lookback in lookback_periods:\n",
    "\n",
    "        means =[]\n",
    "        stds =[]\n",
    "        for ticker, group in df.groupby('Ticker'):\n",
    "\n",
    "            group = group.sort_values('Date')\n",
    "            closes = group['Close_Lag_1'].reset_index(drop=True)\n",
    "            mean_vals = closes.rolling(window=lookback,min_periods=lookback).mean()\n",
    "            std_vals = closes.rolling(window=lookback, min_periods=lookback).std()\n",
    "\n",
    "            means.append(pd.Series(mean_vals.values, index=group.index))\n",
    "            stds.append(pd.Series(std_vals.values, index=group.index))\n",
    "\n",
    "\n",
    "        df[f'Simple_Mean_{lookback}'] =pd.concat(means)\n",
    "        df[f'Simple_Std_{lookback}'] =pd.concat(stds)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4287fb12",
   "metadata": {},
   "source": [
    "Additional engineered features include daily returns and differences in Close prices, giving us insight into market momentum from the last close price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4114a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we added the return of the previous day\n",
    "def add_returns(df):\n",
    "    df['return_since_last_entry'] = df['Close_Lag_1']/df['Close_Lag_2'] -1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704fdb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we added the Close price difference of the previous day\n",
    "def add_diff(df):\n",
    "    df['diff_since_last_entry'] = df['Close_Lag_1']-df['Close_Lag_2']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e71a410",
   "metadata": {},
   "source": [
    "Binary signal features are also introduced to indicate the presence of dividends and stock splits, rather then just their numerical value. The idea was to help the model detect event-driven price changes, just on the events not on their actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf336586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since so many rows for dividends and stock splits are zero we wanted to add a signal for it to the df\n",
    "def add_cat_signals(df, cols):\n",
    "    for col in cols:\n",
    "        new_col = f\"{col}_signal\"\n",
    "        df[new_col] = (df[col] != 0).astype('int32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c37e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_lags(df, [1,2,3,4,5,10,20,30,40,50,60,70,80,90,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3078849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_weighted_mean_std_numba(df,[2,3,4,5,10,20,30,40,50,60,70,80,90,100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a05a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_simple_mean_std(df,[2,3,4,5,10,20,30,40,50,60,70,80,90,100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8b671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_returns(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_diff(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64982573",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_cat_signals(df, ['Dividends','Stock Splits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6198035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(df.columns)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fc6378",
   "metadata": {},
   "source": [
    "We delete the Date column since we have the same information in the Day_number column. Moreover we cannot include the volumne column since this value also captures information that is not avialable before market closing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcf327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since date is in the datetime type and the information of time is preserved in the Day_number column we can delete it\n",
    "df = df.drop('Date', axis=1)\n",
    "#we need to delete Volumne since it contains information to some extend about the same day. therefore we cannot use it. remeber we still have a lagged volumne in the df\n",
    "df = df.drop('Volume', axis=1)\n",
    "# print(df['Stock Splits_signal'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('pr13_all_cols.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4898e550",
   "metadata": {},
   "source": [
    "# oder hier nicht samplen sondern das letzte Jahr nehmen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbbb803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to visualize dependecies between features and grasp their behavior we make the assumption that ~10k rows = 10% of the dataset is sufficient\n",
    "# df_fs = df.sample(frac=0.1, random_state=42)\n",
    "df_fs = df[df['Day_Number']>=8036] # we assume that the last time period is more important rather then randomly sampling\n",
    "print(df[df['Day_Number']==8036]) #first day in 2022 --> 9273 rows ~10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6400836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_fs[df_fs['Close'] > 1800]\n",
    "filtered_df\n",
    "\n",
    "df_fs = df_fs[df_fs['Ticker'] != 'CMG']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2921b08b",
   "metadata": {},
   "source": [
    "# Outlier CMG ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69211e9d",
   "metadata": {},
   "source": [
    "These scatterplots visualize Close vs. Year, Month, Day, and DayOfWeek, to explore how Close behaves over time and identify time-related patterns in stock prices. We observe higher Close prices for an increase in year. Moreover we see that several tickers show gernerally higher Close prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aba3602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we wanted to visualize the behavior of time feature against the target column Close. E.g. find certain days where Close prices are higher then normal\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df_fs, x=\"Close\", y=\"Year\",hue=\"Ticker\", legend=False)\n",
    "plt.title('Close vs Year')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df_fs, x=\"Close\", y=\"Year\",hue=\"Ticker\", legend=False)\n",
    "plt.xlim(0,400)\n",
    "plt.title('Close vs Year (Zoomed)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df_fs, x=\"Close\", y=\"Month\", hue=\"Ticker\", legend=False)\n",
    "plt.title('Close vs Month')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df_fs, x=\"Close\", y=\"Day\", hue=\"Ticker\", legend=False)\n",
    "plt.title('Close vs Day')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df_fs, x=\"Close\", y=\"DayOfWeek\", hue=\"Ticker\", legend=False)\n",
    "plt.title('Close vs DayOfWeek')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ccd274",
   "metadata": {},
   "source": [
    "We plot relationships between numerical features (Close, Dividends, Stock Splits, and their binary signal columns) to see how these variables interact. Since we only have 17 stocksplit in the entire dataset and we sampled 10% we have 0 stocksplits in our subset. For dividends we see higher dividends for higher stock prices. The rest of the features does not really provides more insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05955b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'pairplot of close, div, stock split, theri signal' remember there is only 17 stocksplits and around 900 dividends\n",
    "g= sns.pairplot(df_fs[['Close', 'Dividends', 'Stock Splits', 'Dividends_signal', 'Stock Splits_signal','Ticker']], hue = 'Ticker', diag_kind=\"kde\", corner=True)\n",
    "g._legend.remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee43a61",
   "metadata": {},
   "source": [
    "We now explore how categorical features (Country, Industry, and Ticker) relate to Close and want to observe their distribution. We see higher Close prices for usa, certain tickers (ADBE,NFLX etc.) and industries like technology and entertainment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9869fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in the following we visualize the categorical columns Ticker , Country and Industry against the Close price\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.stripplot(x='Country', y='Close', data=df_fs, jitter=True, palette='deep')\n",
    "plt.title('Country vs Close')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.stripplot(x='Industry_Tag', y='Close', data=df_fs, jitter=True, palette='muted')\n",
    "plt.title('Industry vs Close')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.stripplot(x='Ticker', y='Close', data=df_fs, jitter=True, palette='bright')\n",
    "plt.title('Ticker vs Close')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd10ed98",
   "metadata": {},
   "source": [
    "Are their any interactions between the categroical features? We basically observe the same trends we saw above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e467870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the pairplot only support numerical varibles we decided to encode our categorical columns in a ordinal way, to be able to plot a pairplot\n",
    "# Encode categorical features\n",
    "plt.figure(figsize=(35, 35))\n",
    "encoder = OrdinalEncoder()\n",
    "encoded_features = encoder.fit_transform(df_fs[['Ticker', 'Industry_Tag','Country']])\n",
    "encoded_df = pd.DataFrame(encoded_features , columns=['Ticker','Industry_Tag', 'Country'])\n",
    "encoded_df['Close'] = df_fs['Close' ].values\n",
    "g = sns.pairplot(encoded_df, diag_kind=\"kde\", corner=True)\n",
    "\n",
    "#from ordinal label axis back to names--> due to the number of ticker/industies --> not all names displayable\n",
    "original_labels = df_fs[['Ticker', 'Industry_Tag','Country']]\n",
    "for i, col in enumerate(['Ticker', 'Industry_Tag','Country']):\n",
    "    mapping = dict(zip(\n",
    "        encoded_df[col].unique(),\n",
    "        original_labels[col].astype(str).unique()))\n",
    "    for ax in g.axes.flat:\n",
    "        if ax is not None:\n",
    "            # Xlabels\n",
    "            if ax.get_xlabel() == col:\n",
    "                ticks = ax.get_xticks()\n",
    "                labels = [mapping.get(tick, '') for tick in ticks]\n",
    "                ax.set_xticklabels(labels, rotation=45)\n",
    "            #ylabels\n",
    "            if ax.get_ylabel() == col:\n",
    "                ticks = ax.get_yticks()\n",
    "                labels = [mapping.get(tick, '') for tick in ticks]\n",
    "                ax.set_yticklabels(labels, rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331729ba",
   "metadata": {},
   "source": [
    "We also wanted to visualize the dependece of Close by return (relative change) and price difference (absolute change) from the last day. We find that returns are normally/t-student distributed, as expected (with some outlying tickers), while the price difference since the last entry is also centered around 0 with some tickers coloring the outer parts of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd61f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df_fs[['return_since_last_entry','diff_since_last_entry','Close', 'Ticker']], hue = \"Ticker\", diag_kind=\"kde\", corner=True)\n",
    "g._legend.remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c05a19",
   "metadata": {},
   "source": [
    "As we expect that the most recent features have the most impact on the Close price we plotted them individually and observed a strong linear relationship between the last Close price and our target. Volume and the number of days since the last entry do not seem to give much information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d01211",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df_fs[['Close_Lag_1','Volume_Lag_1','Days_Since_Lag_1','Close', 'Ticker']], hue = \"Ticker\", diag_kind=\"kde\", corner=True)\n",
    "g._legend.remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40090f23",
   "metadata": {},
   "source": [
    "Moreover we wanted to see the influence of the weighting in comparison to the simple mean and std for different lookback periods. For reasons of simplicity and clarity we are only showing the lookback period = 2. What we can observe in these pairplots, that the weigthing seems to increase the linearity of the model, since the distribution of mean and std seems to be less spread if the prices are time weighted. Again Volume and the number of days since the last entry seem not to provide much insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a9d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_list = [2]#,3,4,5,10,20,30,40,50,60,70,80,90,100] # we looked at all added features but for structure we just show lag 2 here. uncomment the remaining lags to see all\n",
    "for lookback in lookback_list:\n",
    "    print('Status: Lag', lookback)\n",
    "    feature_list = f'Simple_Mean_{lookback}'\n",
    "    g = sns.pairplot(df_fs[[f'Close_Lag_{lookback}',f'Volume_Lag_{lookback}',f'Days_Since_Lag_{lookback}',f'Simple_Mean_{lookback}',f'Simple_Std_{lookback}',f'Weighted_Mean_{lookback}',f'Weighted_Std_{lookback}','Close', 'Ticker']], hue = \"Ticker\", diag_kind=\"kde\", corner=True)\n",
    "    g._legend.remove()\n",
    "    plt.show() \n",
    "\n",
    "# what we see is that the weighted average shows cleaner linear distribution to the Close price, also the std is less spread --> prefer weighted over simple\n",
    "# we observe that Days Since and volumne seem not to carry much information. Still left the last Volume Day Since to check later with correlation and mututual information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc92ec31",
   "metadata": {},
   "source": [
    "As a result we dropped all simple mean and std column since they would just include more noise than the weighted mean and std for the same lookback period. For Volumne and number of days since the last entry we decided to keep the last entry since this might be the most important out of all the volumne and number of days since the last entry to check their importance later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dde6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fs =df_fs.drop(columns=[col for  col in df_fs.columns if 'Simple' in col]) #all simple dropped\n",
    "days_cols = [col  for col in df_fs.columns if 'Days_Since' in col]\n",
    "df_fs = df_fs.drop(columns=days_cols[1:] ) # all but the first day since dropped\n",
    "volume_cols = [col for col in df_fs.columns if 'Volume_Lag' in col]\n",
    "df_fs =df_fs.drop( columns=volume_cols[1:]) # all but the first volumne dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9ef164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine categorical and numerical columns here\n",
    "\n",
    "column_types = pd.DataFrame({\n",
    "    'Column Name': df_fs.columns,\n",
    "    'Data Type': df_fs.dtypes.values\n",
    "})\n",
    "print(column_types)\n",
    "\n",
    "numerical_cols = df_fs.select_dtypes(include='float64').columns.tolist() \n",
    "numerical_cols.append('Day_Number')\n",
    "\n",
    "all_columns = df_fs.columns.tolist()\n",
    "\n",
    "categorical_cols = list(set(all_columns) - set(numerical_cols))\n",
    "\n",
    "print(categorical_cols)\n",
    "print(numerical_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d844b32",
   "metadata": {},
   "source": [
    "As discussed in the office hour, we tried to find the best features by looking at their importance before modeling, not assuming a tree or any other underlying model. Overall the idea was to check mutual information (A. Kraskov, H. Stogbauer and P. Grassberger, “Estimating mutual information”. Phys. Rev. E 69, 2004.) and correlation to determine the important feature. Since we are training a global model we wanted to check for the global correlation and mutual information first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfbdd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot global correlation heatmap and show the order of features based on global correlation\n",
    "#Stocksplits does not have any entries in our sample--> empty/NaN\n",
    "df_numeric = df_fs[numerical_cols]\n",
    "plt.figure(figsize=(35, 35))\n",
    "sns.heatmap(df_numeric.corr(), annot=True,fmt=\".2f\", cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "corr_matrix = df_numeric.corr()\n",
    "target_corr = corr_matrix['Close' ].drop('Close').sort_values(ascending=False)\n",
    "# print(target_corr)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(corr_matrix[['Close']].sort_values(by='Close' ,ascending=False),annot=True, cmap='coolwarm',linewidths=0.5)\n",
    "plt.title(f\"Correlation with '{'Close'}'\",fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab160010",
   "metadata": {},
   "source": [
    "We see low absolute correlation of  Dividends, StockSplits, difference, returns, volumne and number of days since last entry witht the close price. We can reasonably assume low correlation aswell for the signal for stock splits and dividends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaeff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the same as above but with mutual information\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "df_numeric = df_fs[numerical_cols]\n",
    "df_numeric = df_numeric.dropna() #mutual information cannot handle Nans --> drop the max 100 rows for each ticker with Nans\n",
    "print('we dropped:',df_fs.shape[0]-df_numeric.shape[0],'rows with Nans')\n",
    "\n",
    "X = df_numeric.drop(columns=['Close'])\n",
    "y = df_numeric['Close']\n",
    "\n",
    "mi_scores = mutual_info_regression(X, y, random_state=0)\n",
    "\n",
    "mi_df = pd.DataFrame({'Feature': X.columns, 'Mutual Information': mi_scores})\n",
    "mi_df = mi_df.sort_values(by='Mutual Information', ascending=False).set_index('Feature')\n",
    "\n",
    "\n",
    "# print(mi_df)\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "sns.heatmap(mi_df[['Mutual Information']], annot=True, fmt=\".3f\", cmap='viridis', linewidths=0.5)\n",
    "plt.title(f\"Mutual Information with '{'Close'}'\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238ffeb5",
   "metadata": {},
   "source": [
    "As before for correlation we observe low mutual information for dividends, StockSplits, price difference, returns, volumne and number of days since last entry witht the close price. We can reasonably assume low correlation aswell for the signal for stock splits and dividends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41764185",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=30 #shows the top 30 features per ticker\n",
    "def analyze_correlation(df_fs, n_features=n):\n",
    "    numeric_cols = numerical_cols\n",
    "    if 'Close' not in numeric_cols:\n",
    "        raise ValueError(\"'Close' must bein your DataFrame and numeric.\")\n",
    "\n",
    "    if 'Ticker' not in df_fs.columns:\n",
    "        raise ValueError(\"'Ticker' must be in your DataFrame.\")\n",
    "    \n",
    "    results = []\n",
    "    for ticker, group in df_fs.groupby('Ticker'):\n",
    "        if len(group) < 2: #group must have more than two entries\n",
    "            continue\n",
    "\n",
    "        group_numeric = group[numeric_cols] #only numeric cols\n",
    "        corr_matrix = group_numeric.corr()\n",
    "        target_corr = corr_matrix['Close'].drop('Close').sort_values(ascending=False)\n",
    "        top_features = target_corr.head(n_features)\n",
    "        \n",
    "        for rank, (feature_name, correlation) in enumerate(top_features.items(), start=1):\n",
    "            results.append({'Ticker': ticker,'Feature': feature_name, 'Correlation': correlation, 'Rank': rank\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "results_df = analyze_correlation(df_fs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb86d7",
   "metadata": {},
   "source": [
    "Following a different logic, we assumed that a high mutual information and correlation for a single Stock (Ticker) would result in a good global performance. Therefore we checked the top features for each ticker and plotted their frequency distribution over all tickers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f67f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_features, counts =np.unique(results_df['Feature'],return_counts=True)\n",
    "feature_distribution = pd.DataFrame({'Feature': unique_features, 'Count': counts})\n",
    "\n",
    "threshold = 10 #every ticker has its 30 best features in the results_df. we only show the features that are mentioned more than 10 times in the top 30 features per ticker\n",
    "\n",
    "# Filter features that meet the threshold condition\n",
    "filtered_distribution = feature_distribution[feature_distribution['Count']>threshold]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(filtered_distribution['Feature'],filtered_distribution['Count'], color='skyblue')\n",
    "plt.xlabel('Feature ')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Feature Distribution (Filtered by Count Threshold)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a1127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_mutual_information(df_fs, n_features=n):\n",
    "    numeric_cols = numerical_cols\n",
    "    if 'Close' not in numeric_cols:\n",
    "        raise ValueError(\"'Close' must bein your DataFrame and numeric.\")\n",
    "\n",
    "    if 'Ticker' not in df_fs.columns:\n",
    "        raise ValueError(\"'Ticker' must be in your DataFrame.\")\n",
    "    \n",
    "    results = []\n",
    "    for ticker, group in df_fs.groupby('Ticker'):\n",
    "        if len(group) < 2: #group must have more than two entries\n",
    "            continue\n",
    "\n",
    "        group_numeric = group[numeric_cols] #only numeric cols\n",
    "        group_numeric = group_numeric.dropna() #mutual information cannot handle Nans --> drop the max 100 rows for each ticker with Nans\n",
    "        X = group_numeric.drop(columns=['Close'])\n",
    "        y = group_numeric['Close']\n",
    "        mutual_info_values = mutual_info_regression(X, y, random_state=0)\n",
    "        mutual_info_dict = dict(zip(X.columns, mutual_info_values))\n",
    "        top_features = pd.Series(mutual_info_values, index=X.columns).sort_values(ascending=False).head(n_features)\n",
    "\n",
    "        \n",
    "        for rank, (feature_name, mi) in enumerate(top_features.items(), start=1):\n",
    "            results.append({'Ticker': ticker,'Feature': feature_name, 'Mutual Information': mi, 'Rank': rank\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "mi_results_df = analyze_mutual_information(df_fs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7091beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_features, counts = np.unique(mi_results_df['Feature'], return_counts=True)\n",
    "feature_distribution = pd.DataFrame({'Feature': unique_features, 'Count': counts})\n",
    "\n",
    "threshold = 10 #every ticker has its 30 best features in the results_df. we only show the features that are mentioned more than 10 times in the top 30 features per ticker\n",
    "\n",
    "filtered_distribution = feature_distribution[feature_distribution['Count'] > threshold]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(filtered_distribution['Feature'], filtered_distribution['Count'], color='skyblue')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Feature Distribution (Filtered by Count Threshold)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b214fc3a",
   "metadata": {},
   "source": [
    "Putting it all together we wanted to indtroduce a metric that is equally weighting the global and ticker specific correlation and mutual information values and return the \"importance\" of the individual feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8a5efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_with_combined_score(global_corr_series, global_mi_df, df_fs, numerical_cols, n_features=30, weights=None):\n",
    "    if weights is None:\n",
    "        weights = {\n",
    "            'global_corr': 0.25,\n",
    "            'global_mi': 0.25,\n",
    "            'local_corr': 0.25,\n",
    "            'local_mi': 0.25\n",
    "        }\n",
    "\n",
    "    global_corr_df = pd.DataFrame(global_corr_series)\n",
    "    global_corr_df.columns = ['Correlation']\n",
    "    global_corr_df['Norm_Corr'] = MinMaxScaler().fit_transform(global_corr_df[['Correlation']])\n",
    "\n",
    "    # Global\n",
    "    global_mi_df['Norm_MI'] = MinMaxScaler().fit_transform(global_mi_df[['Mutual Information']])\n",
    "\n",
    "    #per ticker\n",
    "    corr_results_df = analyze_correlation(df_fs, n_features=n_features)\n",
    "    mi_results_df = analyze_mutual_information(df_fs, n_features=n_features)\n",
    "\n",
    "    local_corr_norm = pd.DataFrame()\n",
    "    local_mi_norm = pd.DataFrame()\n",
    "    \n",
    "    for feature, group in corr_results_df.groupby('Feature'):\n",
    "        local_corr_norm.loc[feature, 'Corr_Mean'] = group['Correlation'].mean()\n",
    "\n",
    "    local_corr_norm['Norm_Corr'] = MinMaxScaler().fit_transform(local_corr_norm[['Corr_Mean']])\n",
    "    \n",
    "        \n",
    "    for feature, group in mi_results_df.groupby('Feature'):\n",
    "            local_mi_norm.loc[feature, 'MI_Mean'] = group['Mutual Information'].mean()\n",
    "\n",
    "    local_mi_norm['Norm_MI'] = MinMaxScaler().fit_transform(local_mi_norm[['MI_Mean']])\n",
    "\n",
    "    features = global_corr_df.index\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for feature in features:\n",
    "        gc = global_corr_df.loc[feature,'Norm_Corr'] if feature in global_corr_df.index else 0\n",
    "        gm = global_mi_df.loc[feature, 'Norm_MI'] if feature in global_mi_df.index else 0\n",
    "        lc = local_corr_norm.loc[feature,'Norm_Corr'] if feature in local_corr_norm.index else 0\n",
    "        lm = local_mi_norm.loc[feature, 'Norm_MI'] if feature in local_mi_norm.index else 0\n",
    "\n",
    "        final_score = (\n",
    "            weights['global_corr'] * gc +\n",
    "            weights['global_mi'] * gm +\n",
    "            weights['local_corr'] * lc +\n",
    "            weights['local_mi'] * lm\n",
    "        )\n",
    "\n",
    "        results.append((feature, final_score))\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42abcd99",
   "metadata": {},
   "source": [
    "As in the individual and global correlation and mututal information results, we observe almost the same order of features with the last Close Price on the first place, the weighted close priced in order until a lookback of 10. After that we see weighted means and lagged closed prices with higher lookback periods. After that we observe std with high lookback periods and the Day_Number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34caa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = select_features_with_combined_score(\n",
    "    target_corr,\n",
    "    mi_df,\n",
    "    df_fs,\n",
    "    numerical_cols,\n",
    "    n_features=len(numerical_cols)\n",
    ")\n",
    "\n",
    "for feature, score in top_features[:10]:\n",
    "    print(f\"{feature}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e119b",
   "metadata": {},
   "source": [
    "As a result we decided to include the weighted means and lagged Close prices until a lookback period/lag of 10, plus a short/medium/ and long term weighted mean and std. On top we included the all time features and the catgeorical features Ticker, Industry and Country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196fc8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection is done with visualization, correlation and mutual information\n",
    "df = df[[ 'Ticker', 'Industry_Tag', 'Country',\n",
    "       'Year', 'Month', 'Day', 'DayOfWeek', \n",
    "       'Close_Lag_1', 'Close_Lag_2','Close_Lag_3', 'Close_Lag_4', 'Close_Lag_5','Close_Lag_10',\n",
    "       'Weighted_Mean_2', 'Weighted_Mean_3', 'Weighted_Mean_4','Weighted_Mean_5', 'Weighted_Mean_10',\n",
    "       'Weighted_Mean_30', 'Weighted_Std_30',\n",
    "       'Weighted_Mean_60', 'Weighted_Std_60',\n",
    "       'Weighted_Mean_100','Weighted_Std_100',\n",
    "       'Day_Number','Close']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ae6a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('pr13_final_selected.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5db8935",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "You should implement AT LEAST FIVE approaches we covered, and tune of at least two hyperparameters of each approach.\n",
    "Do not forget that you should split your data.\n",
    "You should do model selection and tuning using cross validation on the train set, avoiding data leakage.\n",
    "Explain and justify what is the metric you are using for model selection and tuning. If your data is imbalanced, consider using techniques for data balancing.\n",
    "\n",
    "Separately, you should train a neural network. Visualize the training and validation loss. Discuss the network performance\n",
    "\n",
    "In model selection, make sure when you compare different models and approaches that you compare them on the same dataset, though different transformations could be applied to the comparison dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ad10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_time(df,test_percentage=0.2):\n",
    "    df = df.sort_values(by='Day_Number')\n",
    "    n_samples = df.shape[0]\n",
    "    test_size =int(n_samples *test_percentage)\n",
    "    train_df =df.iloc[:-test_size] \n",
    "    test_df =df.iloc[-test_size:]\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1e5503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_y(df):\n",
    "    X = df.drop(columns=[\"Close\"])\n",
    "    y = df[\"Close\"]\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de28439",
   "metadata": {},
   "source": [
    "# Here we take only data from 2020 onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ec2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we assume that the last years are the most important ones and very old data just introduces more noise. We have included a period of growth, before and after covid, and we observe the behaviour of a crisis (COVID).\n",
    "df = df[df['Day_Number']>=7304] #first day of 2020 --> 19879 rows \n",
    "split_day = 7304\n",
    "\n",
    "#set X,y for train and test set + set cv sets since they all have to be equal for all models to compare them in the end\n",
    "train_df, test_df = split_data_by_time(df)\n",
    "X_train, y_train = create_X_y(train_df)\n",
    "X_test, y_test = create_X_y(test_df)\n",
    "\n",
    "numerical_cols = df.select_dtypes(include='float64').columns.tolist() \n",
    "numerical_cols.append('Day_Number')\n",
    "\n",
    "\n",
    "all_columns = df.columns.tolist()\n",
    "categorical_cols = list(set(all_columns) - set(numerical_cols))\n",
    "\n",
    "numerical_cols.remove(\"Close\")\n",
    "\n",
    "\n",
    "# print(categorical_cols)\n",
    "# print(numerical_cols)\n",
    "df = df.sort_values(by='Day_Number')\n",
    "cv = TimeSeriesSplit(n_splits=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8243ce8",
   "metadata": {},
   "source": [
    "## Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb689d3f",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7d5e11",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa8def",
   "metadata": {},
   "source": [
    "## Tree Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4426c516",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c01b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_numeric_preprocessor = Pipeline([ \n",
    "    (\"scaler\", StandardScaler())\n",
    "])                  \n",
    "\n",
    "xgb_categorical_preprocessor = Pipeline([\n",
    "    (\"onehot\", OneHotEncoder(sparse_output=False, drop='first',handle_unknown='infrequent_if_exist'))\n",
    "])\n",
    "\n",
    "xgb_preprocessor = ColumnTransformer([\n",
    "    (\"numerical\",xgb_numeric_preprocessor, numerical_cols),\n",
    "    (\"categorical\",xgb_categorical_preprocessor, categorical_cols)\n",
    "    \n",
    "], remainder='passthrough')\n",
    "\n",
    "\n",
    "xgb_pipe = Pipeline([\n",
    "    ('preprocessor',xgb_preprocessor),\n",
    "    ('regressor',  XGBRegressor(random_state=0))\n",
    "])\n",
    "\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'preprocessor__numerical__scaler':[StandardScaler(), 'passthrough'],\n",
    "    'regressor__learning_rate': sp_uniform(loc=0.01, scale=0.2),\n",
    "    'regressor__max_depth': [2,3, 5, 7, 10],\n",
    "    'regressor__n_estimators': [300,500,700,1000,1200]\n",
    "}\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "xgb_grid_search = RandomizedSearchCV(xgb_pipe, param_distributions=xgb_param_grid, cv=cv, random_state=0, n_jobs=-1,n_iter=15,error_score='raise' ,scoring='neg_root_mean_squared_error')\n",
    "xgb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "print(xgb_pipe.get_params())\n",
    "xgb_grid_search.fit(X_train,y_train)\n",
    "xgb_best_params = xgb_grid_search.best_params_\n",
    "print('Best parameters:',xgb_best_params)\n",
    "\n",
    "with open(f'best_xgb_model_{split_day}.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_grid_search.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f7ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plot_tree(xgb_grid_search.best_estimator_.named_steps['regressor'], num_trees=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c245d43",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f38c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_numeric_preprocessor = Pipeline([ \n",
    "    (\"scaler\", StandardScaler())\n",
    "])                  \n",
    "\n",
    "cat_preprocessor = ColumnTransformer([\n",
    "    (\"numerical\", cat_numeric_preprocessor, numerical_cols),\n",
    "    (\"categorical\", 'passthrough',categorical_cols)\n",
    "], remainder='passthrough')\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    ('preprocessor', cat_preprocessor),\n",
    "    ('regressor',  CatBoostRegressor(random_state=0, cat_features=list(range(len(numerical_cols), len(numerical_cols) + len(categorical_cols))))) #order changes after preprocessor\n",
    "])\n",
    "\n",
    "cat_param_grid = {\n",
    "    'preprocessor__numerical__scaler':[StandardScaler(), 'passthrough'],\n",
    "    'regressor__learning_rate': sp_uniform(loc=0.01, scale=0.2),\n",
    "    'regressor__depth': [3, 5, 7, 10],\n",
    "    'regressor__iterations': [300, 500, 700, 1000,1200]\n",
    "}\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "cat_grid_search = RandomizedSearchCV(cat_pipe, param_distributions=cat_param_grid, cv=cv, random_state=0, n_jobs=-1, n_iter=15, error_score='raise', scoring='neg_root_mean_squared_error')\n",
    "\n",
    "\n",
    "\n",
    "print(cat_pipe.get_params())\n",
    "cat_grid_search.fit(X_train,y_train)\n",
    "cat_best_params = cat_grid_search.best_params_\n",
    "print('Best parameters:',cat_best_params)\n",
    "\n",
    "with open(f'best_cat_model_{split_day}.pkl', 'wb') as f:\n",
    "    pickle.dump(cat_grid_search.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68518983",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e55831",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_numeric_preprocessor = Pipeline([ \n",
    "    (\"scaler\", StandardScaler())\n",
    "])                  \n",
    "\n",
    "rf_categorical_preprocessor = Pipeline([\n",
    "    (\"onehot\", OneHotEncoder(sparse_output=False, drop='first',handle_unknown='infrequent_if_exist'))\n",
    "])  \n",
    "\n",
    "rf_preprocessor = ColumnTransformer([\n",
    "    (\"numerical\", rf_numeric_preprocessor, numerical_cols),\n",
    "    (\"categorical\", rf_categorical_preprocessor, categorical_cols)\n",
    "], remainder='passthrough')\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    ('preprocessor', rf_preprocessor),\n",
    "    ('regressor',  RandomForestRegressor(random_state=0))\n",
    "])\n",
    "\n",
    "rf_param_grid = {\n",
    "    'preprocessor__numerical__scaler':[StandardScaler(), 'passthrough'],\n",
    "    'regressor__n_estimators': [100, 200, 300, 400, 500, 600],\n",
    "    'regressor__max_depth': [4, 5, 6, 7, None],\n",
    "    'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rf_grid_search = RandomizedSearchCV(rf_pipe, param_distributions=rf_param_grid, cv=cv, random_state=0, n_jobs=-1, n_iter=15, error_score='raise', scoring='neg_root_mean_squared_error')\n",
    "\n",
    "X_train_cleaned = X_train.dropna()\n",
    "y_train_cleaned = y_train[X_train_cleaned.index]\n",
    "\n",
    "print(rf_pipe.get_params())\n",
    "rf_grid_search.fit(X_train_cleaned, y_train_cleaned)\n",
    "rf_best_params = rf_grid_search.best_params_\n",
    "print('Best parameters:',rf_best_params)\n",
    "\n",
    "with open(f'best_rf_model_{split_day}.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_grid_search.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbece5c",
   "metadata": {},
   "source": [
    "## Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd089ec3",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e1d66a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('preprocessing', ColumnTransformer(transformers=[('num', StandardScaler(),\n",
      "                                 ['Year', 'Day', 'Close_Lag_1', 'Close_Lag_2',\n",
      "                                  'Close_Lag_3', 'Close_Lag_4', 'Close_Lag_5',\n",
      "                                  'Close_Lag_10', 'Weighted_Mean_2',\n",
      "                                  'Weighted_Mean_3', 'Weighted_Mean_4',\n",
      "                                  'Weighted_Mean_5', 'Weighted_Mean_10',\n",
      "                                  'Weighted_Mean_30', 'Weighted_Std_30',\n",
      "                                  'Weighted_Mean_60', 'Weighted_Std_60',\n",
      "                                  'Weighted_Mean_100', 'Weighted_Std_100',\n",
      "                                  'Day_Number']),\n",
      "                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n",
      "                                 ['Industry_Tag', 'Country', 'DayOfWeek',\n",
      "                                  'Month'])])), ('knn', KNeighborsRegressor())], 'transform_input': None, 'verbose': False, 'preprocessing': ColumnTransformer(transformers=[('num', StandardScaler(),\n",
      "                                 ['Year', 'Day', 'Close_Lag_1', 'Close_Lag_2',\n",
      "                                  'Close_Lag_3', 'Close_Lag_4', 'Close_Lag_5',\n",
      "                                  'Close_Lag_10', 'Weighted_Mean_2',\n",
      "                                  'Weighted_Mean_3', 'Weighted_Mean_4',\n",
      "                                  'Weighted_Mean_5', 'Weighted_Mean_10',\n",
      "                                  'Weighted_Mean_30', 'Weighted_Std_30',\n",
      "                                  'Weighted_Mean_60', 'Weighted_Std_60',\n",
      "                                  'Weighted_Mean_100', 'Weighted_Std_100',\n",
      "                                  'Day_Number']),\n",
      "                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n",
      "                                 ['Industry_Tag', 'Country', 'DayOfWeek',\n",
      "                                  'Month'])]), 'knn': KNeighborsRegressor(), 'preprocessing__force_int_remainder_cols': True, 'preprocessing__n_jobs': None, 'preprocessing__remainder': 'drop', 'preprocessing__sparse_threshold': 0.3, 'preprocessing__transformer_weights': None, 'preprocessing__transformers': [('num', StandardScaler(), ['Year', 'Day', 'Close_Lag_1', 'Close_Lag_2', 'Close_Lag_3', 'Close_Lag_4', 'Close_Lag_5', 'Close_Lag_10', 'Weighted_Mean_2', 'Weighted_Mean_3', 'Weighted_Mean_4', 'Weighted_Mean_5', 'Weighted_Mean_10', 'Weighted_Mean_30', 'Weighted_Std_30', 'Weighted_Mean_60', 'Weighted_Std_60', 'Weighted_Mean_100', 'Weighted_Std_100', 'Day_Number']), ('cat', OneHotEncoder(handle_unknown='ignore'), ['Industry_Tag', 'Country', 'DayOfWeek', 'Month'])], 'preprocessing__verbose': False, 'preprocessing__verbose_feature_names_out': True, 'preprocessing__num': StandardScaler(), 'preprocessing__cat': OneHotEncoder(handle_unknown='ignore'), 'preprocessing__num__copy': True, 'preprocessing__num__with_mean': True, 'preprocessing__num__with_std': True, 'preprocessing__cat__categories': 'auto', 'preprocessing__cat__drop': None, 'preprocessing__cat__dtype': <class 'numpy.float64'>, 'preprocessing__cat__feature_name_combiner': 'concat', 'preprocessing__cat__handle_unknown': 'ignore', 'preprocessing__cat__max_categories': None, 'preprocessing__cat__min_frequency': None, 'preprocessing__cat__sparse_output': True, 'knn__algorithm': 'auto', 'knn__leaf_size': 30, 'knn__metric': 'minkowski', 'knn__metric_params': None, 'knn__n_jobs': None, 'knn__n_neighbors': 5, 'knn__p': 2, 'knn__weights': 'uniform'}\n",
      "Fitting 5 folds for each of 198 candidates, totalling 990 fits\n",
      "Best parameters: {'knn__n_neighbors': 43, 'knn__weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "df_knn = df.copy()\n",
    "df_knn['target'] = np.log(df_knn['Close'].shift(-1)) - np.log(df_knn['Close'])\n",
    "df_knn = df_knn.dropna(subset=['target'])\n",
    "df_knn = df_knn.dropna() #194 rows dropped\n",
    "\n",
    "train_df_knn, test_df_knn = split_data_by_time(df_knn)\n",
    "X_train_knn = train_df_knn.drop(columns=['target', 'Close'])\n",
    "y_train_knn = train_df_knn['target']\n",
    "X_test_knn = test_df_knn.drop(columns=['target', 'Close'])\n",
    "y_test_knn = test_df_knn['target']\n",
    "\n",
    "cat_knn = ['Industry_Tag', 'Country', 'DayOfWeek', 'Month']\n",
    "num_knn = [\n",
    "    'Year', 'Day',\n",
    "    'Close_Lag_1', 'Close_Lag_2', 'Close_Lag_3', 'Close_Lag_4', 'Close_Lag_5', 'Close_Lag_10',\n",
    "    'Weighted_Mean_2', 'Weighted_Mean_3', 'Weighted_Mean_4', 'Weighted_Mean_5', 'Weighted_Mean_10',\n",
    "    'Weighted_Mean_30', 'Weighted_Std_30', 'Weighted_Mean_60', 'Weighted_Std_60',\n",
    "    'Weighted_Mean_100', 'Weighted_Std_100', 'Day_Number'\n",
    "]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_knn),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_knn)\n",
    "])\n",
    "\n",
    "knn_pipe = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"knn\", KNeighborsRegressor(n_neighbors=5))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"knn__n_neighbors\": list(range(1, 100)),\n",
    "    \"knn__weights\": [\"uniform\", \"distance\"]\n",
    "}\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "knn_grid_search = GridSearchCV(\n",
    "    knn_pipe,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(knn_pipe.get_params())\n",
    "knn_grid_search.fit(X_train_knn, y_train_knn)\n",
    "knn_best_params = knn_grid_search.best_params_\n",
    "print('Best parameters:', knn_best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de60dd2",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "After selecting your final model, which could be a compromise of performance, interpretability and complexity, you should evaluate its performance on the test set. \n",
    "You might have tuned your model using a certain metric, but now you should describe the model performance using all relevant metrics. \n",
    "If you have some business insight, why a certain metric is relevant, you should explain it. \n",
    "Construct a suitable baseline to benchmark your result and to put them in the context.\n",
    "Discuss your results, do they seem good enough to be used in practice? If not, what should be improved. Discuss what type of errors is your model making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251fdddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_shap_feature_importance(estimator, X):\n",
    "    print(\"Calculating SHAP values...\")\n",
    "    explainer = shap.TreeExplainer(estimator.named_steps['regressor'])\n",
    "    feature_names = estimator.named_steps['preprocessor'].get_feature_names_out()\n",
    "    X_processed = estimator.named_steps['preprocessor'].transform(X)\n",
    "    shap_values = explainer.shap_values(X_processed)\n",
    "\n",
    "    print(\"Plotting SHAP summary (bar)...\")\n",
    "    shap.summary_plot(shap_values, X_processed, feature_names=feature_names,plot_type=\"bar\")\n",
    "\n",
    "    print(\"Plotting SHAP summary (beeswarm)...\")\n",
    "    shap.summary_plot(shap_values, X_processed,feature_names=feature_names, plot_type=\"dot\")\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_permutation_feature_importance(model, X, y, n_repeats=30, random_state=0):\n",
    "\n",
    "    print(\"Calculating Permutation Importance...\")\n",
    "    perm_result =permutation_importance(model, X,y,n_repeats=n_repeats, random_state=random_state, n_jobs=-1)\n",
    "    sorted_idx = perm_result.importances_mean.argsort()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.barh(X.columns[sorted_idx],perm_result.importances_mean[sorted_idx])\n",
    "    plt.xlabel(\"Permutation Importance\")\n",
    "    plt.title(\"Feature Importance with Permutation\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5782586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, benchmark_column='Close_Lag_1'):\n",
    "\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    #model metrics\n",
    "    mse_test =mean_squared_error(y_test, y_test_pred)\n",
    "    mae_test =mean_absolute_error(y_test, y_test_pred)\n",
    "    mape_test= mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "    R_2_test = r2_score(y_test, y_test_pred)\n",
    "    adj_R2_test = 1- ((1-R_2_test)*(X_test.shape[0] - 1)/(X_test.shape[0]- X_test.shape[1] -1))\n",
    "    \n",
    "    #benchmark metrics\n",
    "    y_pred_benchmark = X_test[benchmark_column]\n",
    "    mse_benchmark = mean_squared_error(y_test, y_pred_benchmark)\n",
    "    mae_benchmark = mean_absolute_error(y_test, y_pred_benchmark)\n",
    "    mape_benchmark = mean_absolute_percentage_error(y_test, y_pred_benchmark)\n",
    "    R_2_benchmark = r2_score(y_test, y_pred_benchmark)\n",
    "    adj_R2_benchmark = 1 - ((1 -R_2_benchmark) *(X_test.shape[0] -1) /(X_test.shape[0]-X_test.shape[1] - 1))\n",
    "    \n",
    "    print(\"---- Model Evaluation ----\")\n",
    "    print(\"Mean Squared Error on test set:\",mse_test)\n",
    "    print(\"Root Mean Squared Error on test set:\",np.sqrt(mse_test))\n",
    "    print(\"Mean Absolute Error on test set:\",mae_test)\n",
    "    print(\"Mean Absolute Percentage Error on test set:\",mape_test)\n",
    "    print(\"R^2 on test set:\", R_2_test)\n",
    "    print(\"Adjusted R^2 on test set:\", adj_R2_test)\n",
    "    \n",
    "    print(\"---- Benchmark Comparison ----\")\n",
    "    print('Benchmark:',benchmark_column)\n",
    "    print(\"Mean Squared Error on benchmark:\",mse_benchmark)\n",
    "    print(\"Root Mean Squared Error on benchmark:\",np.sqrt(mse_benchmark))\n",
    "    print(\"Mean Absolute Error on benchmark:\",mae_benchmark)\n",
    "    print(\"Mean Absolute Percentage Error on benchmark:\",mape_benchmark)\n",
    "    print(\"R^2 on benchmark:\", R_2_benchmark)\n",
    "    print(\"Adjusted R^2 on benchmark:\", adj_R2_benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f14349",
   "metadata": {},
   "source": [
    "## Model interpretation\n",
    "\n",
    "Use at least two different techniques for model interpretability. Discuss what are the most important features of your model, and how they impact the model performance. Pick a few examples of errors that your model is making, and check which features lead to thess errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2025",
   "language": "python",
   "name": "ml2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
