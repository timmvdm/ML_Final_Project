{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a36b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_ticker(df, test_percentage=0.2):\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing datasets based on the given percentage \n",
    "    for each ticker.\n",
    "    \n",
    "    Args:\n",
    "    - df (DataFrame): DataFrame containing 'DayNumber', 'Ticker', and other columns like 'Price'.\n",
    "    - test_percentage (float): Percentage of data to be used for testing. Defaults to 0.2 (20% for testing, 80% for training).\n",
    "    \n",
    "    Returns:\n",
    "    - train_df (DataFrame): Training dataset with all tickers.\n",
    "    - test_df (DataFrame): Testing dataset with all tickers.\n",
    "    \"\"\"\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "\n",
    "    # Group by Ticker and iterate through each group\n",
    "    for ticker, group in df.groupby('Ticker'):\n",
    "        # Sort each ticker's group by DayNumber before splitting\n",
    "        group = group.sort_values(by='Day_Number')\n",
    "\n",
    "        n_samples = group.shape[0]\n",
    "        test_size = int(n_samples * test_percentage)\n",
    "        print(f\"Ticker: {ticker}, Samples: {n_samples}, Test size: {test_size}\")\n",
    "        \n",
    "        # Split the group into train and test datasets\n",
    "        train = group.iloc[:-test_size]  # Take the first part as training\n",
    "        test = group.iloc[-test_size:]   # Take the last part as testing\n",
    "\n",
    "        # Append to the respective lists\n",
    "        train_data.append(train)\n",
    "        test_data.append(test)\n",
    "        \n",
    "    # Concatenate all the individual train and test data for all tickers\n",
    "    train_df = pd.concat(train_data).sort_values(by='Day_Number')  # Sort by DayNumber after concatenating\n",
    "    test_df = pd.concat(test_data).sort_values(by='Day_Number')    # Sort by DayNumber after concatenating\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming the dataset has a 'ticker' column and 'Close' column\n",
    "tickers = df['ticker'].unique()\n",
    "\n",
    "# Define the preprocessing steps\n",
    "numeric_preprocessor = Pipeline([ \n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_preprocessor = Pipeline([\n",
    "    (\"onehot\", OneHotEncoder(sparse_output=False, drop='first', handle_unknown='infrequent_if_exist'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"numerical\", numeric_preprocessor, numerical_cols),\n",
    "    (\"categorical\", categorical_preprocessor, categorical_cols)\n",
    "], remainder='passthrough')\n",
    "\n",
    "# Define the XGBoost regressor model\n",
    "xgb_model = XGBRegressor(random_state=0)\n",
    "\n",
    "# Define parameter grid for RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'regressor__learning_rate': sp_uniform(loc=0.03, scale=0.07),\n",
    "    'regressor__max_depth': [4, 5, 6, 7],\n",
    "    'regressor__n_estimators': [300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "}\n",
    "\n",
    "# Perform TimeSeriesSplit\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Initialize a dictionary to store models for each ticker\n",
    "ticker_models = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    # Filter data for the current ticker\n",
    "    ticker_data = df[df['ticker'] == ticker]\n",
    "    X_ticker = ticker_data.drop(columns=['Close', 'ticker'])  # Features\n",
    "    y_ticker = ticker_data['Close']  # Target (Close price)\n",
    "    \n",
    "    # Define the pipeline for this specific ticker\n",
    "    pipe = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', xgb_model)\n",
    "    ])\n",
    "    \n",
    "    # Perform randomized search for the best hyperparameters\n",
    "    grid_search = RandomizedSearchCV(pipe, param_distributions=param_grid, cv=cv, random_state=0, n_jobs=-1, n_iter=15, \n",
    "                                     error_score='raise', scoring='neg_root_mean_squared_error')\n",
    "    \n",
    "    grid_search.fit(X_ticker, y_ticker)\n",
    "    \n",
    "    # Store the best model for the ticker\n",
    "    ticker_models[ticker] = grid_search.best_estimator_\n",
    "\n",
    "    print(f\"Best model for ticker {ticker}:\")\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "# Now you can use these models for predictions on new data:\n",
    "predictions = {}\n",
    "for ticker, model in ticker_models.items():\n",
    "    ticker_data = df[df['ticker'] == ticker]  # Get the data for the ticker\n",
    "    X_ticker = ticker_data.drop(columns=['Close', 'ticker'])\n",
    "    y_ticker = ticker_data['Close']\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_ticker)\n",
    "    \n",
    "    # Store predictions\n",
    "    predictions[ticker] = y_pred\n",
    "    \n",
    "    # You can evaluate the model as well\n",
    "    mse = mean_squared_error(y_ticker, y_pred)\n",
    "    print(f\"Mean Squared Error for {ticker}: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fad2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of combining predictions by averaging\n",
    "combined_predictions = []\n",
    "\n",
    "for ticker, model in ticker_models.items():\n",
    "    ticker_data = df[df['ticker'] == ticker]\n",
    "    X_ticker = ticker_data.drop(columns=['Close', 'ticker'])\n",
    "    \n",
    "    # Get predictions from the model\n",
    "    y_pred = model.predict(X_ticker)\n",
    "    \n",
    "    # Append to the combined predictions list\n",
    "    combined_predictions.extend(y_pred)\n",
    "\n",
    "# You can now evaluate the combined predictions or use them for further analysis\n",
    "combined_predictions = np.array(combined_predictions)\n",
    "\n",
    "# For example, calculate the overall MSE for all tickers together\n",
    "y_true = df['Close'].values\n",
    "combined_mse = mean_squared_error(y_true, combined_predictions)\n",
    "print(f\"Overall Mean Squared Error: {combined_mse}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
